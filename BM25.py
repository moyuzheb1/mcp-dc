import re
import nltk
import math
import json
import matplotlib.pyplot as plt
from collections import defaultdict
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# ---------------------- 1. åˆå§‹åŒ–NLTKèµ„æºï¼ˆé¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¸‹è½½ï¼‰----------------------
try:
    stopwords.words('english')
    wordnet.synsets('test')
except LookupError:
    nltk.download('stopwords')
    nltk.download('wordnet')
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger')

# ---------------------- 2. é…ç½®å‚æ•°ï¼ˆä»…éœ€ä¿®æ”¹è¿™3å¤„ï¼ï¼‰----------------------
INPUT_JSON = "papers.json"  # ä½ çš„JSONæ–‡ä»¶åï¼ˆåŒæ–‡ä»¶å¤¹ä¸‹ï¼Œå¦‚"my_papers.json"ï¼‰
OUTPUT_JSON = "bm25_screening_results.json"  # è¾“å‡ºç»“æœæ–‡ä»¶å
QUERY = "large language model"  # ä½ çš„åˆç­›å…³é”®è¯ï¼ˆè‹±æ–‡ï¼‰

# BM25å‚æ•°ï¼ˆè‹±æ–‡æ‘˜è¦æ¨èå€¼ï¼Œæ— éœ€æ”¹åŠ¨ï¼‰
K1 = 0.9  # è¯é¢‘é¥±å’Œç³»æ•°
B = 0.5   # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–ç³»æ•°
SELECTION_THRESHOLD = 0.3  # ç­›é€‰é˜ˆå€¼ï¼ˆå¯æ ¹æ®å¾—åˆ†åˆ†å¸ƒå›¾è°ƒæ•´ï¼‰

# ---------------------- 3. å·¥å…·åˆå§‹åŒ– ----------------------
lemmatizer = WordNetLemmatizer()  # è‹±æ–‡è¯å½¢è¿˜åŸå™¨
STOPWORDS = set(stopwords.words('english'))  # è‹±æ–‡åœç”¨è¯è¡¨

# ---------------------- 4. æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼ˆè‹±æ–‡é€‚é…ï¼‰----------------------
def clean_text_en(text):
    """è‹±æ–‡æ–‡æœ¬æ¸…æ´—ï¼šå»æ ‡ç‚¹ã€æ•°å­—ã€å°å†™åŒ–"""
    text = re.sub(r'[^\w\s]', '', text)  # å»æ ‡ç‚¹
    text = re.sub(r'\d+', '', text)      # å»æ•°å­—
    return text.lower().strip()

def get_wordnet_pos(tag):
    """è¯æ€§æ ‡ç­¾è½¬æ¢ï¼ˆç”¨äºç²¾å‡†è¯å½¢è¿˜åŸï¼‰"""
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    return wordnet.NOUN

def tokenize_en(text):
    """è‹±æ–‡åˆ†è¯+åœç”¨è¯è¿‡æ»¤+è¯å½¢è¿˜åŸ"""
    tokens = word_tokenize(text)  # åˆ†è¯
    pos_tags = nltk.pos_tag(tokens)  # è¯æ€§æ ‡æ³¨
    # è¿‡æ»¤æ— æ•ˆè¯å¹¶è¿˜åŸ
    return [
        lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))
        for word, tag in pos_tags
        if word not in STOPWORDS and len(word) > 1
    ]

def expand_keywords_en(query):
    """å…³é”®è¯æ‰©å±•ï¼ˆåŸºäºWordNetè·å–è¿‘ä¹‰è¯ï¼Œæå‡å¬å›ç‡ï¼‰"""
    expanded = tokenize_en(query)
    for word in tokenize_en(query):
        for syn in wordnet.synsets(word):
            expanded.extend([lemma.name() for lemma in syn.lemmas()])
    return list(set(expanded))  # å»é‡

# ---------------------- 5. BM25æ ¸å¿ƒç®—æ³• ----------------------
def build_bm25_index(tokenized_docs):
    """æ„å»ºBM25ç´¢å¼•ï¼šç»Ÿè®¡è¯é¢‘ã€æ–‡æ¡£é•¿åº¦ç­‰"""
    doc_freqs = defaultdict(int)  # è¯å‡ºç°çš„æ–‡æ¡£æ•°
    doc_lengths = []              # æ¯ç¯‡æ–‡æ¡£çš„è¯æ•°
    term_freqs = []               # æ¯ç¯‡æ–‡æ¡£çš„è¯é¢‘å­—å…¸

    for doc in tokenized_docs:
        doc_len = len(doc)
        doc_lengths.append(doc_len)
        freq = defaultdict(int)
        for word in doc:
            freq[word] += 1
        term_freqs.append(freq)
        # ç»Ÿè®¡æ–‡æ¡£é¢‘ç‡ï¼ˆå»é‡ï¼‰
        for word in set(doc):
            doc_freqs[word] += 1

    avgdl = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 1
    return doc_freqs, doc_lengths, avgdl, term_freqs

def calculate_bm25(query, doc_freqs, doc_lengths, avgdl, term_freqs):
    """è®¡ç®—æ¯ç¯‡æ–‡æ¡£çš„BM25å¾—åˆ†"""
    N = len(doc_lengths)
    tokenized_query = expand_keywords_en(query)
    scores = []

    # è®¡ç®—IDFï¼ˆè¯çš„ç¨€æœ‰åº¦ï¼‰
    idf = {
        word: math.log((N - doc_freqs.get(word, 0) + 0.5) / (doc_freqs.get(word, 0) + 0.5) + 1)
        for word in set(tokenized_query)
    }

    # è®¡ç®—å•ç¯‡æ–‡æ¡£å¾—åˆ†
    for i in range(N):
        doc_len = doc_lengths[i]
        score = 0.0
        doc_freq = term_freqs[i]
        for word in tokenized_query:
            tf = doc_freq.get(word, 0)
            if tf == 0:
                continue
            # BM25æ ¸å¿ƒå…¬å¼
            denominator = tf + K1 * (1 - B + B * (doc_len / avgdl))
            score += idf[word] * (tf * (K1 + 1)) / denominator
        scores.append(score)
    return scores

# ---------------------- 6. JSONæ•°æ®è¯»å†™ ----------------------
def read_json_data(file_path):
    """è¯»å–å«id/title/abstractçš„JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # æ ¡éªŒå¿…å¡«å­—æ®µ
        required_fields = {"id", "title", "abstract"}
        for item in data:
            if not required_fields.issubset(item.keys()):
                raise ValueError(f"JSONç¼ºå°‘å¿…å¡«å­—æ®µï¼æŸæ¡æ•°æ®å­—æ®µï¼š{list(item.keys())}")
        print(f"âœ… æˆåŠŸè¯»å– {len(data)} ç¯‡è®ºæ–‡ï¼ˆå­—æ®µï¼šid/title/abstractï¼‰")
        return data
    except FileNotFoundError:
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶ {file_path}ï¼Œè¯·ç¡®è®¤æ–‡ä»¶åœ¨åŒæ–‡ä»¶å¤¹ä¸‹")
    except json.JSONDecodeError:
        raise ValueError("âŒ JSONæ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹")

def save_results(data, cleaned_abstracts, tokenized_abstracts, scores):
    """ä¿å­˜ç­›é€‰ç»“æœåˆ°JSONï¼ˆä¿ç•™åŸå§‹å­—æ®µ+å¤„ç†ç»“æœï¼‰"""
    results = []
    for i, item in enumerate(data):
        results.append({
            "id": item["id"],
            "title": item["title"],
            "original_abstract": item["abstract"],
            "cleaned_abstract": cleaned_abstracts[i],
            "tokenized_abstract": tokenized_abstracts[i],
            "bm25_score": round(scores[i], 4),
            "is_selected": 1 if scores[i] > SELECTION_THRESHOLD else 0
        })
    # æŒ‰BM25å¾—åˆ†é™åºæ’åº
    results.sort(key=lambda x: x["bm25_score"], reverse=True)
    # ä¿å­˜æ–‡ä»¶
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {OUTPUT_JSON}")
    return results

# ---------------------- 7. ç»“æœå¯è§†åŒ–ä¸è¾“å‡º ----------------------
def visualize_score_distribution(scores):
    """ç»˜åˆ¶BM25å¾—åˆ†åˆ†å¸ƒå›¾ï¼ˆè¾…åŠ©è°ƒæ•´é˜ˆå€¼ï¼‰"""
    plt.figure(figsize=(8, 4))
    plt.hist(scores, bins=10, color='#2E86AB', alpha=0.7, edgecolor='black')
    plt.xlabel('BM25 Score')
    plt.ylabel('Number of Papers')
    plt.title('BM25 Score Distribution (English Abstracts)')
    plt.axvline(x=SELECTION_THRESHOLD, color='red', linestyle='--', label=f'Threshold: {SELECTION_THRESHOLD}')
    plt.legend()
    plt.tight_layout()
    plt.show()

def print_top_results(results, top_n=5):
    """æ‰“å°å¾—åˆ†æœ€é«˜çš„Nç¯‡è®ºæ–‡ï¼ˆå¿«é€ŸæŸ¥çœ‹ï¼‰"""
    print(f"\nğŸ† Top {top_n} é«˜ç›¸å…³è®ºæ–‡ï¼š")
    for i, res in enumerate(results[:top_n]):
        print(f"\nRank {i+1} | Score: {res['bm25_score']}")
        print(f"ID: {res['id']}")
        print(f"Title: {res['title']}")
        print(f"Abstract (first 100 chars): {res['original_abstract'][:100]}...")

# ---------------------- 8. ä¸»æµç¨‹ï¼ˆä¸€é”®è¿è¡Œï¼‰----------------------
if __name__ == "__main__":
    print("=== å¼€å§‹BM25è‹±æ–‡æ–‡çŒ®åˆç­› ===")
    # æ­¥éª¤1ï¼šè¯»å–JSONæ•°æ®
    raw_data = read_json_data(INPUT_JSON)
    # æ­¥éª¤2ï¼šæå–æ‘˜è¦å¹¶é¢„å¤„ç†
    abstracts = [item["abstract"] for item in raw_data]
    cleaned_abs = [clean_text_en(abs) for abs in abstracts]
    tokenized_abs = [tokenize_en(abs) for abs in cleaned_abs]
    print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼ˆæ¸…æ´—+åˆ†è¯+è¯å½¢è¿˜åŸï¼‰")
    # æ­¥éª¤3ï¼šè®¡ç®—BM25å¾—åˆ†
    doc_freqs, doc_lengths, avgdl, term_freqs = build_bm25_index(tokenized_abs)
    bm25_scores = calculate_bm25(QUERY, doc_freqs, doc_lengths, avgdl, term_freqs)
    print("âœ… BM25å¾—åˆ†è®¡ç®—å®Œæˆ")
    # æ­¥éª¤4ï¼šä¿å­˜ç»“æœ
    final_results = save_results(raw_data, cleaned_abs, tokenized_abs, bm25_scores)
    # æ­¥éª¤5ï¼šå¯è§†åŒ–+æ‰“å°Topç»“æœ
    visualize_score_distribution(bm25_scores)
    print_top_results(final_results, top_n=5)
    # ç»Ÿè®¡ç­›é€‰ç»“æœ
    selected_count = sum([1 for res in final_results if res["is_selected"] == 1])
    print(f"\n=== ç­›é€‰å®Œæˆ ===")
    print(f"æ€»è®ºæ–‡æ•°ï¼š{len(final_results)}")
    print(f"å…¥é€‰è®ºæ–‡æ•°ï¼š{selected_count}")