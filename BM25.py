# ç¬¬ä¸€æ­¥ï¼šå…ˆæ·»åŠ æ—¥å¿—é…ç½®ï¼ˆç¡®ä¿æ‰€æœ‰æ­¥éª¤éƒ½æœ‰è¾“å‡ºï¼‰
import sys
import logging

# é…ç½®æ—¥å¿—ï¼šå¼ºåˆ¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œä¸é™é»˜ä»»ä½•ä¿¡æ¯
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    stream=sys.stdout  # ç¡®ä¿è¾“å‡ºåˆ°å‘½ä»¤è¡Œï¼Œä¸è¢«éšè—
)
logger = logging.getLogger(__name__)

# ç¬¬äºŒæ­¥ï¼šå¯¼å…¥ä¾èµ–ï¼ˆæ¯æ­¥éƒ½åŠ æ—¥å¿—ï¼Œçœ‹æ˜¯å¦å¡åœ¨å¯¼å…¥ï¼‰
logger.info("å¼€å§‹åŠ è½½ä¾èµ–æ¨¡å—...")
try:
    import re
    import nltk
    import math
    import json
    import matplotlib.pyplot as plt
    from collections import defaultdict
    from nltk.corpus import stopwords, wordnet
    from nltk.tokenize import word_tokenize
    from nltk.stem import WordNetLemmatizer
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    from typing import List, Dict, Any
    import uvicorn
    import os
    import platform
    logger.info("âœ… æ‰€æœ‰ä¾èµ–æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    logger.error(f"âŒ ä¾èµ–æ¨¡å—åŠ è½½å¤±è´¥ï¼šç¼ºå°‘ {e.name}ï¼ˆè¯·è¿è¡Œ pip install {e.name}ï¼‰")
    sys.exit(1)

# æ£€æŸ¥Pythonç‰ˆæœ¬ï¼ˆFastAPIè¦æ±‚3.7+ï¼‰
logger.info(f"å½“å‰Pythonç‰ˆæœ¬ï¼š{platform.python_version()}")
if sys.version_info < (3, 7):
    logger.error("âŒ Pythonç‰ˆæœ¬è¿‡ä½ï¼è¯·ä½¿ç”¨Python 3.7åŠä»¥ä¸Šç‰ˆæœ¬")
    sys.exit(1)

# ---------------------- æ ¸å¿ƒé…ç½® ----------------------
DEFAULT_K1 = 0.9
DEFAULT_B = 0.5
DEFAULT_THRESHOLD = 0.3
PAPERS_JSON_PATH = "papers.json"
API_HOST = "0.0.0.0"
API_PORT = 2625  # ç›®æ ‡ç«¯å£

# ---------------------- NLTKèµ„æºåˆå§‹åŒ–ï¼ˆå¼ºåˆ¶æ—¥å¿—è¾“å‡ºï¼‰----------------------
logger.info("å¼€å§‹åˆå§‹åŒ–NLTKèµ„æº...")
try:
    # æµ‹è¯•èµ„æºæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™ä¸‹è½½ï¼ˆæ·»åŠ è¶…æ—¶æ§åˆ¶ï¼‰
    stopwords.words('english')
    wordnet.synsets('test')
    logger.info("âœ… NLTKèµ„æºå·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½")
except LookupError:
    logger.info("âš ï¸  æœªæ‰¾åˆ°NLTKèµ„æºï¼Œå¼€å§‹è‡ªåŠ¨ä¸‹è½½ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è”ç½‘ï¼‰...")
    try:
        nltk.download('stopwords', quiet=True)
        nltk.download('wordnet', quiet=True)
        nltk.download('punkt', quiet=True)
        nltk.download('averaged_perceptron_tagger', quiet=True)
        logger.info("âœ… NLTKèµ„æºä¸‹è½½å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ NLTKèµ„æºä¸‹è½½å¤±è´¥ï¼š{str(e)}ï¼ˆè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼‰")
        sys.exit(1)

# ---------------------- å·¥å…·åˆå§‹åŒ– ----------------------
lemmatizer = WordNetLemmatizer()
STOPWORDS = set(stopwords.words('english'))
logger.info("âœ… å·¥å…·åˆå§‹åŒ–å®Œæˆ")

# ---------------------- è¯»å–è®ºæ–‡æ•°æ®ï¼ˆæ·»åŠ è·¯å¾„æ—¥å¿—ï¼‰----------------------
def load_papers_from_file(file_path: str = PAPERS_JSON_PATH) -> List[Dict[str, str]]:
    logger.info(f"å¼€å§‹è¯»å–è®ºæ–‡æ–‡ä»¶ï¼š{os.path.abspath(file_path)}")
    logger.info(f"å½“å‰å·¥ä½œç›®å½•ï¼š{os.getcwd()}")  # æ‰“å°å½“å‰ç›®å½•ï¼Œæ–¹ä¾¿ç”¨æˆ·æ’æŸ¥æ–‡ä»¶ä½ç½®
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆå½“å‰ç›®å½•ï¼š{os.getcwd()}ï¼‰")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            papers = json.load(f)
        
        required_fields = {"id", "title", "abstract"}
        for idx, paper in enumerate(papers):
            if not required_fields.issubset(paper.keys()):
                missing = required_fields - set(paper.keys())
                raise ValueError(f"ç¬¬ {idx+1} ç¯‡è®ºæ–‡ç¼ºå°‘å­—æ®µï¼š{missing}ï¼ˆIDï¼š{paper.get('id', 'æœªçŸ¥')}ï¼‰")
        
        if len(papers) == 0:
            raise ValueError("è®ºæ–‡æ–‡ä»¶ä¸ºç©º")
        
        logger.info(f"âœ… æˆåŠŸè¯»å– {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    except json.JSONDecodeError:
        raise ValueError("JSONæ ¼å¼é”™è¯¯ï¼ˆè¯·ç”¨ https://json.cn/ æ ¡éªŒï¼‰")
    except Exception as e:
        raise RuntimeError(f"è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")

# é¢„åŠ è½½è®ºæ–‡
GLOBAL_PAPERS = []
try:
    GLOBAL_PAPERS = load_papers_from_file()
except Exception as e:
    logger.error(f"âš ï¸  è®ºæ–‡æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼ˆå¯åŠ¨åAPIä¼šè¿”å›è¯¥é”™è¯¯ï¼‰")
    GLOBAL_PAPERS = []  # ç»§ç»­å¯åŠ¨æœåŠ¡ï¼Œè®©ç”¨æˆ·é€šè¿‡APIæŸ¥çœ‹è¯¦æƒ…

# ---------------------- æ•°æ®é¢„å¤„ç†å‡½æ•° ----------------------
def clean_text_en(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    return text.lower().strip()

def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    return wordnet.NOUN

def tokenize_en(text):
    tokens = word_tokenize(text)
    pos_tags = nltk.pos_tag(tokens)
    return [
        lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))
        for word, tag in pos_tags
        if word not in STOPWORDS and len(word) > 1
    ]

def expand_keywords_en(query):
    expanded = tokenize_en(query)
    for word in tokenize_en(query):
        for syn in wordnet.synsets(word):
            expanded.extend([lemma.name() for lemma in syn.lemmas()])
    return list(set(expanded))

# ---------------------- BM25æ ¸å¿ƒç®—æ³• ----------------------
def build_bm25_index(tokenized_docs):
    doc_freqs = defaultdict(int)
    doc_lengths = []
    term_freqs = []

    for doc in tokenized_docs:
        doc_len = len(doc)
        doc_lengths.append(doc_len)
        freq = defaultdict(int)
        for word in doc:
            freq[word] += 1
        term_freqs.append(freq)
        for word in set(doc):
            doc_freqs[word] += 1

    avgdl = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 1
    return doc_freqs, doc_lengths, avgdl, term_freqs

def calculate_bm25(query, doc_freqs, doc_lengths, avgdl, term_freqs, k1=DEFAULT_K1, b=DEFAULT_B):
    N = len(doc_lengths)
    tokenized_query = expand_keywords_en(query)
    scores = []

    idf = {
        word: math.log((N - doc_freqs.get(word, 0) + 0.5) / (doc_freqs.get(word, 0) + 0.5) + 1)
        for word in set(tokenized_query)
    }

    for i in range(N):
        doc_len = doc_lengths[i]
        score = 0.0
        doc_freq = term_freqs[i]
        for word in tokenized_query:
            tf = doc_freq.get(word, 0)
            if tf == 0:
                continue
            denominator = tf + k1 * (1 - b + b * (doc_len / avgdl))
            score += idf[word] * (tf * (k1 + 1)) / denominator
        scores.append(score)
    return scores

# ---------------------- ç»“æœå¤„ç†å‡½æ•°ï¼ˆç®€åŒ–è¿”å›å­—æ®µï¼‰----------------------
def process_papers(query: str, k1: float = DEFAULT_K1, b: float = DEFAULT_B) -> List[Dict[str, Any]]:
    if not GLOBAL_PAPERS:
        raise ValueError("æœªåŠ è½½åˆ°æœ‰æ•ˆè®ºæ–‡æ•°æ®ï¼Œè¯·æ£€æŸ¥ï¼š1. papers.jsonæ˜¯å¦åœ¨å½“å‰ç›®å½• 2. JSONæ ¼å¼æ˜¯å¦æ­£ç¡® 3. æ˜¯å¦åŒ…å«id/title/abstractå­—æ®µ")
    
    abstracts = [paper["abstract"] for paper in GLOBAL_PAPERS]
    cleaned_abs = [clean_text_en(abs_text) for abs_text in abstracts]
    tokenized_abs = [tokenize_en(abs_text) for abs_text in cleaned_abs]
    
    doc_freqs, doc_lengths, avgdl, term_freqs = build_bm25_index(tokenized_abs)
    bm25_scores = calculate_bm25(query, doc_freqs, doc_lengths, avgdl, term_freqs, k1, b)
    
    # æ ¸å¿ƒä¿®æ”¹ï¼šä»…ä¿ç•™ idã€titleã€original_abstract + è¯„åˆ†ç›¸å…³å­—æ®µ
    results = []
    for i, paper in enumerate(GLOBAL_PAPERS):
        results.append({
            "id": paper["id"],  # ä¿ç•™
            "title": paper["title"],  # ä¿ç•™
            "original_abstract": paper["abstract"],  # ä¿ç•™
            "bm25_score": round(bm25_scores[i], 4),  # ä¿ç•™ï¼ˆç”¨äºåˆ¤æ–­ç›¸å…³æ€§å¼ºå¼±ï¼‰
            "is_selected": 1 if bm25_scores[i] > DEFAULT_THRESHOLD else 0  # ä¿ç•™ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦å…¥é€‰ï¼‰
        })
    
    results.sort(key=lambda x: x["bm25_score"], reverse=True)
    return results

# ---------------------- APIæ¥å£å®šä¹‰ï¼ˆåŒæ­¥ç®€åŒ–å“åº”æ¨¡å‹ï¼‰----------------------
app = FastAPI(title="BM25è®ºæ–‡ç­›é€‰API", description="ä»…éœ€ä¼ å…¥æŸ¥è¯¢å…³é”®è¯ï¼Œè¿”å›ç›¸å…³æ€§æ’åºç»“æœï¼ˆç®€åŒ–å­—æ®µï¼‰")

class BM25Request(BaseModel):
    query: str  # å”¯ä¸€å¿…å¡«å‚æ•°
    k1: float = DEFAULT_K1
    b: float = DEFAULT_B

# ç®€åŒ–å“åº”æ¨¡å‹ï¼šæ˜ç¡®è¿”å›å­—æ®µ
class PaperResult(BaseModel):
    id: str
    title: str
    original_abstract: str
    bm25_score: float
    is_selected: int

class BM25Response(BaseModel):
    results: List[PaperResult]  # ç”¨ç®€åŒ–åçš„PaperResultæ¨¡å‹
    total_papers: int
    selected_count: int
    threshold: float = DEFAULT_THRESHOLD

@app.post("/bm25/score", response_model=BM25Response, summary="è·å–è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†")
async def score_papers(request: BM25Request):
    try:
        results = process_papers(query=request.query, k1=request.k1, b=request.b)
        selected_count = sum(1 for res in results if res["is_selected"] == 1)
        return {
            "results": results,
            "total_papers": len(results),
            "selected_count": selected_count,
            "threshold": DEFAULT_THRESHOLD
        }
    except Exception as e:
        logger.error(f"APIå¤„ç†å¤±è´¥ï¼š{str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

# ---------------------- å¯åŠ¨é€»è¾‘ï¼ˆå¼ºåˆ¶æ—¥å¿—è¾“å‡ºï¼‰----------------------
if __name__ == "__main__":
    logger.info("=== BM25è®ºæ–‡ç­›é€‰API å¼€å§‹å¯åŠ¨ ===")
    logger.info(f"ğŸ“¡ æœåŠ¡é…ç½®ï¼š{API_HOST}:{API_PORT}")
    logger.info(f"ğŸ“„ è®ºæ–‡æ–‡ä»¶è·¯å¾„ï¼š{os.path.abspath(PAPERS_JSON_PATH)}")
    logger.info("âš ï¸  å¯åŠ¨åè¯·å‹¿å…³é—­ç»ˆç«¯ï¼ˆå…³é—­å°†åœæ­¢æœåŠ¡ï¼‰")
    logger.info("ğŸ’¡ è®¿é—® http://localhost:2625/docs å¯æµ‹è¯•API")
    
    try:
        # å¯åŠ¨æœåŠ¡ï¼ˆæ·»åŠ æ—¥å¿—å›è°ƒï¼Œç¡®ä¿å¯åŠ¨çŠ¶æ€å¯è§ï¼‰
        uvicorn.run(
            app=app,
            host=API_HOST,
            port=API_PORT,
            log_level="info",
            access_log=False  # å…³é—­è®¿é—®æ—¥å¿—ï¼Œåªä¿ç•™å¯åŠ¨æ—¥å¿—
        )
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥ï¼š{str(e)}")
        # é’ˆå¯¹å¸¸è§é”™è¯¯ç»™å‡ºæç¤º
        if "address already in use" in str(e).lower():
            logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šç«¯å£2625å·²è¢«å ç”¨ï¼Œè¯·å…³é—­å ç”¨ç¨‹åºï¼Œæˆ–ä¿®æ”¹ä»£ç ä¸­API_PORTä¸ºå…¶ä»–ç«¯å£ï¼ˆå¦‚2626ï¼‰")
        elif "permission denied" in str(e).lower():
            logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šæ— æƒé™ä½¿ç”¨è¯¥ç«¯å£ï¼ˆWindowséœ€ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œç»ˆç«¯ï¼ŒLinux/Macéœ€åŠ sudoï¼‰")
        sys.exit(1)