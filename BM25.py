# ç¬¬ä¸€æ­¥ï¼šå…ˆæ·»åŠ æ—¥å¿—é…ç½®
import sys
import logging
import csv
import pickle
import os
import re
import nltk
import math
from collections import defaultdict
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any
import uvicorn
import platform

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

# ---------------------- æ ¸å¿ƒé…ç½® ----------------------
DEFAULT_K1 = 0.9
DEFAULT_B = 0.5
DEFAULT_THRESHOLD = 0.3
PAPERS_CSV_PATH = "papers.csv"
API_HOST = "0.0.0.0"
API_PORT = 2625
# æ–°å¢ï¼šç´¢å¼•ç¼“å­˜æ–‡ä»¶è·¯å¾„
INDEX_CACHE_FILE = "bm25_index.pkl"

# ---------------------- NLTKèµ„æºåˆå§‹åŒ– ----------------------
logger.info("å¼€å§‹åˆå§‹åŒ–NLTKèµ„æº...")
try:
    stopwords.words('english')
    wordnet.synsets('test')
    logger.info("âœ… NLTKèµ„æºå·²å­˜åœ¨")
except LookupError:
    logger.info("âš ï¸  æœªæ‰¾åˆ°NLTKèµ„æºï¼Œå¼€å§‹è‡ªåŠ¨ä¸‹è½½...")
    try:
        nltk.download('stopwords', quiet=True)
        nltk.download('wordnet', quiet=True)
        nltk.download('punkt', quiet=True)
        nltk.download('averaged_perceptron_tagger', quiet=True)
        logger.info("âœ… NLTKèµ„æºä¸‹è½½å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ NLTKèµ„æºä¸‹è½½å¤±è´¥ï¼š{str(e)}")
        sys.exit(1)

# ---------------------- å·¥å…·åˆå§‹åŒ– ----------------------
lemmatizer = WordNetLemmatizer()
STOPWORDS = set(stopwords.words('english'))
logger.info("âœ… å·¥å…·åˆå§‹åŒ–å®Œæˆ")

# ---------------------- å…¨å±€å˜é‡ ----------------------
GLOBAL_PAPERS = []
# æ–°å¢ï¼šå…¨å±€ç´¢å¼•å˜é‡
GLOBAL_BM25_INDEX = None

# ---------------------- ã€ä½ çš„åŸå§‹åŠ è½½å‡½æ•°ã€‘ ----------------------
# è¿™é‡Œè¯·ç²˜è´´ä½ åŸæ¥å¯ä»¥æ­£å¸¸è¿è¡Œçš„ load_papers_from_file å‡½æ•°
# ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘å‡è®¾å®ƒé•¿è¿™æ ·ã€‚å¦‚æœä½ çš„ä¸åŒï¼Œè¯·åŠ¡å¿…æ›¿æ¢æˆä½ çš„ç‰ˆæœ¬ï¼
def load_papers_from_file(file_path: str = PAPERS_CSV_PATH) -> List[Dict[str, str]]:
    logger.info(f"å¼€å§‹è¯»å–è®ºæ–‡æ–‡ä»¶ï¼š{os.path.abspath(file_path)}")
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    papers = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        # å‡è®¾ä½ çš„åˆ—åæ˜¯ 'id', 'title', 'abstract'ï¼Œå¦‚æœä¸æ˜¯ï¼Œè¯·ä¿®æ”¹
        for row in reader:
            papers.append({
                "id": row["id"],
                "title": row["title"],
                "abstract": row["abstract"]
            })
    
    logger.info(f"âœ… æˆåŠŸè¯»å– {len(papers)} ç¯‡è®ºæ–‡")
    return papers

# ---------------------- æ•°æ®é¢„å¤„ç†å’ŒBM25ç®—æ³•ï¼ˆæ— æ”¹åŠ¨ï¼‰ ----------------------
def clean_text_en(text):
    if not text: return ""
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    return text.lower().strip()

def get_wordnet_pos(tag):
    if tag.startswith('J'): return wordnet.ADJ
    elif tag.startswith('V'): return wordnet.VERB
    elif tag.startswith('N'): return wordnet.NOUN
    elif tag.startswith('R'): return wordnet.ADV
    return wordnet.NOUN

def tokenize_en(text):
    if not text: return []
    tokens = word_tokenize(text)
    pos_tags = nltk.pos_tag(tokens)
    return [
        lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))
        for word, tag in pos_tags
        if word not in STOPWORDS and len(word) > 1
    ]

def expand_keywords_en(query):
    expanded = tokenize_en(query)
    for word in expanded.copy():
        try:
            for syn in wordnet.synsets(word)[:2]:
                expanded.extend([lemma.name() for lemma in syn.lemmas()[:3]])
        except:
            continue
    return list(set(expanded))

def build_bm25_index(tokenized_docs):
    logger.info("å¼€å§‹æ„å»ºBM25ç´¢å¼•...")
    doc_freqs = defaultdict(int)
    doc_lengths = []
    term_freqs = []

    for doc in tokenized_docs:
        doc_len = len(doc)
        doc_lengths.append(doc_len)
        freq = defaultdict(int)
        for word in doc:
            freq[word] += 1
        term_freqs.append(freq)
        for word in set(doc):
            doc_freqs[word] += 1

    avgdl = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 1
    logger.info("âœ… BM25ç´¢å¼•æ„å»ºå®Œæˆ")
    return doc_freqs, doc_lengths, avgdl, term_freqs

def calculate_bm25(query, doc_freqs, doc_lengths, avgdl, term_freqs, k1=DEFAULT_K1, b=DEFAULT_B):
    N = len(doc_lengths)
    tokenized_query = expand_keywords_en(query)
    scores = []

    idf = {
        word: math.log((N - doc_freqs.get(word, 0) + 0.5) / (doc_freqs.get(word, 0) + 0.5) + 1)
        for word in set(tokenized_query)
    }

    for i in range(N):
        doc_len = doc_lengths[i]
        score = 0.0
        doc_freq = term_freqs[i]
        for word in tokenized_query:
            tf = doc_freq.get(word, 0)
            if tf == 0: continue
            denominator = tf + k1 * (1 - b + b * (doc_len / avgdl))
            score += idf[word] * (tf * (k1 + 1)) / denominator
        scores.append(score)
    return scores

# ---------------------- ã€æ–°å¢ã€‘ç´¢å¼•åŠ è½½å’Œä¿å­˜å‡½æ•° ----------------------
def save_index(index_data, cache_file=INDEX_CACHE_FILE):
    """å°†ç´¢å¼•æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶"""
    try:
        with open(cache_file, 'wb') as f:
            pickle.dump(index_data, f)
        logger.info(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ° {cache_file}")
    except Exception as e:
        logger.warning(f"âš ï¸  ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")

def load_index(cache_file=INDEX_CACHE_FILE) -> tuple:
    """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•æ•°æ®"""
    if not os.path.exists(cache_file):
        logger.warning(f"âŒ ç´¢å¼•æ–‡ä»¶ {cache_file} ä¸å­˜åœ¨")
        return None
    
    try:
        with open(cache_file, 'rb') as f:
            index_data = pickle.load(f)
        logger.info(f"âœ… å·²ä» {cache_file} åŠ è½½ç´¢å¼•")
        return index_data
    except Exception as e:
        logger.error(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
        os.remove(cache_file) # åˆ é™¤æŸåçš„ç´¢å¼•æ–‡ä»¶
        logger.info("âš ï¸  å·²åˆ é™¤æŸåçš„ç´¢å¼•æ–‡ä»¶ï¼Œå°†é‡æ–°æ„å»º")
        return None

# ---------------------- ã€ä¼˜åŒ–åã€‘ç»“æœå¤„ç†å‡½æ•° ----------------------
def process_papers(query: str, k1: float = DEFAULT_K1, b: float = DEFAULT_B) -> List[Dict[str, Any]]:
    # æ‰“å°æŸ¥è¯¢å‚æ•°
    logger.info(f"æŸ¥è¯¢å‚æ•°: {query}")
    global GLOBAL_BM25_INDEX

    if not GLOBAL_PAPERS:
        raise ValueError("æœªåŠ è½½åˆ°æœ‰æ•ˆè®ºæ–‡æ•°æ®")
    
    # æ ¸å¿ƒä¼˜åŒ–ï¼šå¦‚æœç´¢å¼•å·²åœ¨å†…å­˜ä¸­ï¼Œç›´æ¥ä½¿ç”¨
    if GLOBAL_BM25_INDEX is None:
        # å°è¯•ä»æ–‡ä»¶åŠ è½½ç´¢å¼•
        GLOBAL_BM25_INDEX = load_index()

        # å¦‚æœæ–‡ä»¶ä¸­æ²¡æœ‰ç´¢å¼•ï¼Œåˆ™ç°åœºæ„å»ºå¹¶ä¿å­˜
        if GLOBAL_BM25_INDEX is None:
            logger.info("ç´¢å¼•æœªæ‰¾åˆ°ï¼Œæ­£åœ¨è¿›è¡Œé¦–æ¬¡é¢„å¤„ç†å’Œç´¢å¼•æ„å»ºï¼ˆæ­¤è¿‡ç¨‹ä»…ä¸€æ¬¡ï¼‰...")
            abstracts = [paper["abstract"] for paper in GLOBAL_PAPERS]
            cleaned_abs = [clean_text_en(abs_text) for abs_text in abstracts]
            tokenized_abs = [tokenize_en(abs_text) for abs_text in cleaned_abs]
            GLOBAL_BM25_INDEX = build_bm25_index(tokenized_abs)
            # ä¿å­˜ç´¢å¼•ä¾›ä¸‹æ¬¡ä½¿ç”¨
            save_index(GLOBAL_BM25_INDEX)
    
    # ä½¿ç”¨å†…å­˜ä¸­çš„ç´¢å¼•è¿›è¡Œå¿«é€Ÿè®¡ç®—
    doc_freqs, doc_lengths, avgdl, term_freqs = GLOBAL_BM25_INDEX
    bm25_scores = calculate_bm25(query, doc_freqs, doc_lengths, avgdl, term_freqs, k1, b)
    
    # ç»“æœå¤„ç†
    results = []
    for i, paper in enumerate(GLOBAL_PAPERS):
        results.append({
            "id": paper["id"],
            "title": paper["title"],
            "original_abstract": paper["abstract"],
            "bm25_score": round(bm25_scores[i], 4),
        })
    
    results.sort(key=lambda x: x["bm25_score"], reverse=True)
    return results[:1]

# ---------------------- APIæ¥å£å’Œå¯åŠ¨é€»è¾‘ï¼ˆæ— æ”¹åŠ¨ï¼‰ ----------------------
app = FastAPI(title="BM25è®ºæ–‡ç­›é€‰API", description="ä¼˜åŒ–ç‰ˆï¼šé¢„ç¼“å­˜ç´¢å¼•ï¼Œå®ç°æ¯«ç§’çº§å“åº”")

from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class BM25Request(BaseModel):
    query: str
    k1: float = DEFAULT_K1
    b: float = DEFAULT_B

class PaperResult(BaseModel):
    id: str
    title: str
    original_abstract: str
    bm25_score: float

class BM25Response(BaseModel):
    results: List[PaperResult]
    total_papers: int
    selected_count: int
    threshold: float = DEFAULT_THRESHOLD

@app.post("/bm25/score", response_model=BM25Response, summary="è·å–è®ºæ–‡ç›¸å…³æ€§è¯„åˆ†")
async def score_papers(request: BM25Request):
    # æ‰“å°æŸ¥è¯¢å‚æ•°
    logger.info(f"æŸ¥è¯¢å‚æ•°: {request.query}")
    try:
        results = process_papers(query=request.query, k1=request.k1, b=request.b)
        return {
            "results": results,
            "total_papers": len(GLOBAL_PAPERS),
            "selected_count": len(results),
            "threshold": DEFAULT_THRESHOLD
        }
    except Exception as e:
        logger.error(f"APIå¤„ç†å¤±è´¥ï¼š{str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

def start_server():
    try:
        global GLOBAL_PAPERS
        GLOBAL_PAPERS = load_papers_from_file()
        
        logger.info("=== BM25è®ºæ–‡ç­›é€‰APIï¼ˆä¼˜åŒ–ç‰ˆï¼‰å¼€å§‹å¯åŠ¨ ===")
        logger.info(f"ğŸ“¡ æœåŠ¡é…ç½®ï¼š{API_HOST}:{API_PORT}")
        logger.info(f"ğŸ“„ è®ºæ–‡æ–‡ä»¶è·¯å¾„ï¼š{os.path.abspath(PAPERS_CSV_PATH)}")
        logger.info("ğŸ’¡ é¦–æ¬¡æŸ¥è¯¢å¯èƒ½è¾ƒæ…¢ï¼ˆéœ€æ„å»ºç´¢å¼•ï¼‰ï¼Œåç»­æŸ¥è¯¢å°†ä¸ºæ¯«ç§’çº§")
        logger.info("ğŸ’¡ è®¿é—® http://localhost:2625/docs å¯æµ‹è¯•API")
        
        uvicorn.run(
            app=app,
            host=API_HOST,
            port=API_PORT,
            log_level="info",
            access_log=True,
            reload=False,
            workers=1
        )
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥ï¼š{str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    start_server()