# ç¬¬ä¸€æ­¥ï¼šæ·»åŠ æ—¥å¿—é…ç½®
import sys
import logging
import os
import platform
import csv
import numpy as np
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware  # å¯¼å…¥CORSä¸­é—´ä»¶
from pydantic import BaseModel
import uvicorn
from sentence_transformers import SentenceTransformer, util
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

# ç¬¬äºŒæ­¥ï¼šå¯¼å…¥ä¾èµ–å¹¶æ£€æŸ¥
logger.info("å¼€å§‹åŠ è½½ä¾èµ–æ¨¡å—...")
try:
    # æ ¸å¿ƒä¾èµ–æ£€æŸ¥
    import torch  # sentence-transformersä¾èµ–PyTorch
    logger.info(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
    logger.info("âœ… æ‰€æœ‰ä¾èµ–æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    logger.error(f"âŒ ä¾èµ–æ¨¡å—åŠ è½½å¤±è´¥ï¼šç¼ºå°‘ {e.name}ï¼ˆè¯·è¿è¡Œ pip install {e.name}ï¼‰")
    sys.exit(1)

# æ£€æŸ¥Pythonç‰ˆæœ¬
logger.info(f"å½“å‰Pythonç‰ˆæœ¬ï¼š{platform.python_version()}")
if sys.version_info < (3, 7):
    logger.error("âŒ Pythonç‰ˆæœ¬è¿‡ä½ï¼è¯·ä½¿ç”¨Python 3.7åŠä»¥ä¸Šç‰ˆæœ¬")
    sys.exit(1)

# ---------------------- æ ¸å¿ƒé…ç½® ----------------------
DEFAULT_MODEL = "all-MiniLM-L6-v2"  # è½»é‡çº§Sentence-BERTæ¨¡å‹
DEFAULT_THRESHOLD = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼
PAPERS_CSV_PATH = "papers.csv"  # è®ºæ–‡CSVæ–‡ä»¶è·¯å¾„
API_HOST = "0.0.0.0"
API_PORT = 2378  # ä¸BM25åŒºåˆ†ç«¯å£

# ---------------------- æ¨¡å‹åˆå§‹åŒ– ----------------------
logger.info(f"å¼€å§‹åŠ è½½Sentence-BERTæ¨¡å‹ï¼š{DEFAULT_MODEL}...")
try:
    model = SentenceTransformer("./local_models/all-MiniLM-L6-v2")  # æ›¿æ¢ä¸ºå®é™…æœ¬åœ°æ¨¡å‹è·¯å¾„
    logger.info("âœ… Sentence-BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
except Exception as e:
    logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼ˆè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹åç§°ï¼‰")
    sys.exit(1)

# ---------------------- è¯»å–è®ºæ–‡æ•°æ® ----------------------
def load_papers_from_file(file_path: str = PAPERS_CSV_PATH) -> List[Dict[str, str]]:
    logger.info(f"å¼€å§‹è¯»å–è®ºæ–‡æ–‡ä»¶ï¼š{os.path.abspath(file_path)}")
    logger.info(f"å½“å‰å·¥ä½œç›®å½•ï¼š{os.getcwd()}")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆå½“å‰ç›®å½•ï¼š{os.getcwd()}ï¼‰")
    
    try:
        papers = []
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                papers.append({
                    "id": row["id"],
                    "title": row["title"],
                    "abstract": row["abstract"]
                })
        
        required_fields = {"id", "title", "abstract"}
        for idx, paper in enumerate(papers):
            if not required_fields.issubset(paper.keys()):
                missing = required_fields - set(paper.keys())
                raise ValueError(f"ç¬¬ {idx+1} ç¯‡è®ºæ–‡ç¼ºå°‘å­—æ®µï¼š{missing}ï¼ˆIDï¼š{paper.get('id', 'æœªçŸ¥')}ï¼‰")
        
        if len(papers) == 0:
            raise ValueError("è®ºæ–‡æ–‡ä»¶ä¸ºç©º")
        
        logger.info(f"âœ… æˆåŠŸè¯»å– {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    except Exception as e:
        raise RuntimeError(f"è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{str(e)}")

# é¢„åŠ è½½è®ºæ–‡åŠç”ŸæˆåµŒå…¥
GLOBAL_PAPERS = []
PAPER_EMBEDDINGS = None  # å­˜å‚¨è®ºæ–‡æ‘˜è¦çš„åµŒå…¥å‘é‡

try:
    GLOBAL_PAPERS = load_papers_from_file()
    # ç”Ÿæˆæ‘˜è¦åµŒå…¥
    logger.info("å¼€å§‹ç”Ÿæˆè®ºæ–‡æ‘˜è¦åµŒå…¥ï¼ˆé¦–æ¬¡è¿è¡Œå¯èƒ½è€—æ—¶ï¼‰...")
    abstracts = [paper["abstract"] for paper in GLOBAL_PAPERS]
    PAPER_EMBEDDINGS = model.encode(abstracts, convert_to_tensor=True)
    logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(GLOBAL_PAPERS)} ç¯‡è®ºæ–‡çš„åµŒå…¥å‘é‡")
except Exception as e:
    logger.error(f"âš ï¸  è®ºæ–‡æ•°æ®æˆ–åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼š{str(e)}ï¼ˆå¯åŠ¨åAPIä¼šè¿”å›è¯¥é”™è¯¯ï¼‰")
    GLOBAL_PAPERS = []
    PAPER_EMBEDDINGS = None

# ---------------------- æ•°æ®é¢„å¤„ç†å‡½æ•° ----------------------
def clean_text(text: str) -> str:
    """ç®€å•æ–‡æœ¬æ¸…ç†"""
    import re
    text = re.sub(r'\s+', ' ', text)  # åˆå¹¶ç©ºæ ¼
    return text.strip()

# ---------------------- Sentence-BERTæ ¸å¿ƒç®—æ³• ----------------------
def calculate_similarity(query: str) -> List[float]:
    """è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰è®ºæ–‡æ‘˜è¦çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    if PAPER_EMBEDDINGS is None:
        raise ValueError("è®ºæ–‡åµŒå…¥å‘é‡æœªåˆå§‹åŒ–")
    
    # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
    query_embedding = model.encode(clean_text(query), convert_to_tensor=True)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cos_scores = util.cos_sim(query_embedding, PAPER_EMBEDDINGS)[0]
    return cos_scores.cpu().numpy().tolist()  # è½¬æ¢ä¸ºCPUå¹¶è¿”å›åˆ—è¡¨

# ---------------------- ç»“æœå¤„ç†å‡½æ•° ----------------------
def process_papers(query: str, threshold: float = DEFAULT_THRESHOLD) -> List[Dict[str, Any]]:
    if not GLOBAL_PAPERS or PAPER_EMBEDDINGS is None:
        raise ValueError("æœªåŠ è½½åˆ°æœ‰æ•ˆè®ºæ–‡æ•°æ®ï¼Œè¯·æ£€æŸ¥ï¼š1. papers.csvæ˜¯å¦åœ¨å½“å‰ç›®å½• 2. CSVæ ¼å¼æ˜¯å¦æ­£ç¡® 3. æ˜¯å¦åŒ…å«id/title/abstractå­—æ®µ")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = calculate_similarity(query)
    
    # æ„å»ºç»“æœ
    results = []
    for i, paper in enumerate(GLOBAL_PAPERS):
        score = round(similarities[i], 4)
        results.append({
            "id": paper["id"],
            "title": paper["title"],
            "original_abstract": paper["abstract"],
            "similarity_score": score
        })
    
    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    results.sort(key=lambda x: x["similarity_score"], reverse=True)
    # åªè¿”å›åˆ†æ•°æœ€é«˜çš„ä¸€ç¯‡è®ºæ–‡
    return results[:1]

# ---------------------- APIæ¥å£å®šä¹‰ ----------------------
app = FastAPI(title="Sentence-BERTè®ºæ–‡åŒ¹é…API", description="åŸºäºå¥å­åµŒå…¥çš„è®ºæ–‡ç›¸å…³æ€§åŒ¹é…æ¥å£")

# é…ç½®CORSè·¨åŸŸï¼Œè§£å†³OPTIONSè¯·æ±‚405é”™è¯¯
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å¼€å‘ç¯å¢ƒå…è®¸æ‰€æœ‰æºï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®æŒ‡å®šå…·ä½“åŸŸåï¼‰
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰HTTPæ–¹æ³•ï¼ˆåŒ…æ‹¬OPTIONSé¢„æ£€è¯·æ±‚ï¼‰
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´
)

class SentenceBERTRequest(BaseModel):
    query: str  # æŸ¥è¯¢å…³é”®è¯/å¥å­
    threshold: float = DEFAULT_THRESHOLD  # ç›¸ä¼¼åº¦é˜ˆå€¼

class PaperResult(BaseModel):
    id: str
    title: str
    original_abstract: str
    similarity_score: float

class SentenceBERTResponse(BaseModel):
    results: List[PaperResult]
    total_papers: int
    selected_count: int  # ä¿ç•™å­—æ®µåä½†å®é™…ä»£è¡¨æœ€é«˜åˆ†æ•°è®ºæ–‡æ•°ï¼ˆæ€»æ˜¯1ï¼‰
    threshold: float

@app.post("/sentence-bert/match", response_model=SentenceBERTResponse, summary="è·å–è®ºæ–‡ç›¸ä¼¼åº¦åŒ¹é…ç»“æœ")
async def match_papers(request: SentenceBERTRequest):
    try:
        results = process_papers(query=request.query, threshold=request.threshold)
        # åªè¿”å›åˆ†æ•°æœ€é«˜çš„ä¸€ç¯‡è®ºæ–‡ï¼Œæ‰€ä»¥selected_countæ€»æ˜¯1
        return {
            "results": results,
            "total_papers": len(results),
            "selected_count": 1,  # å› ä¸ºåªè¿”å›æœ€é«˜åˆ†æ•°çš„ä¸€ç¯‡è®ºæ–‡
            "threshold": request.threshold
        }
    except Exception as e:
        logger.error(f"APIå¤„ç†å¤±è´¥ï¼š{str(e)}")
        raise HTTPException(status_code=400, detail=str(e))

# ---------------------- å¯åŠ¨é€»è¾‘ ----------------------
if __name__ == "__main__":
    # Windowsç³»ç»Ÿåˆ‡æ¢äº‹ä»¶å¾ªç¯ï¼ˆè§£å†³è¿æ¥é‡ç½®é—®é¢˜ï¼‰
    if platform.system() == "Windows":
        import asyncio
        from asyncio import WindowsSelectorEventLoopPolicy
        asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())
    
    logger.info("=== Sentence-BERTè®ºæ–‡åŒ¹é…API å¼€å§‹å¯åŠ¨ ===")
    logger.info(f"ğŸ“¡ æœåŠ¡é…ç½®ï¼š{API_HOST}:{API_PORT}")
    logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{DEFAULT_MODEL}")
    logger.info("âš ï¸  å¯åŠ¨åè¯·å‹¿å…³é—­ç»ˆç«¯ï¼ˆå…³é—­å°†åœæ­¢æœåŠ¡ï¼‰")
    logger.info("ğŸ’¡ è®¿é—® http://localhost:2378/docs å¯æµ‹è¯•API")
    
    try:
        uvicorn.run(
            app=app,
            host=API_HOST,
            port=API_PORT,
            log_level="info",
            access_log=True
        )
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥ï¼š{str(e)}")
        if "address already in use" in str(e).lower():
            logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šç«¯å£2378å·²è¢«å ç”¨ï¼Œè¯·å…³é—­å ç”¨ç¨‹åºï¼Œæˆ–ä¿®æ”¹ä»£ç ä¸­API_PORTä¸ºå…¶ä»–ç«¯å£ï¼ˆå¦‚2627ï¼‰")
        elif "permission denied" in str(e).lower():
            logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šæ— æƒé™ä½¿ç”¨è¯¥ç«¯å£ï¼ˆWindowséœ€ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œç»ˆç«¯ï¼ŒLinux/Macéœ€åŠ sudoï¼‰")
        sys.exit(1)