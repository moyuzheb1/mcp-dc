[
  {
    "id": "2511.13529v1",
    "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets",
    "original_abstract": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18% on spontaneous and 4.8% on repeated speech. Diarization experiments yield diarization error rates between 13.05% and 18.26%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.",
    "cleaned_abstract": "the advancement of automatic speech recognition asr has been largely enhanced by extensive datasets in highresource languages while languages such as hungarian remain underrepresented due to limited spontaneous and conversational corpora to address this gap we introduce two new datasets  bealarge and beadialogue  constructed from the previously unprocessed portions of the hungarian speech corpus named bea bealarge extends beabase with  hours of spontaneous speech from  speakers enriched with detailed segmentlevel metadata beadialogue comprising  hours of spontaneous conversations is a hungarian speech corpus featuring natural dialogues partitioned into speakerindependent subsets supporting research in conversational asr and speaker diarization we establish reproducible baselines on these datasets using publicly available asr models with the finetuned fast conformer model achieving word error rates as low as  on spontaneous and  on repeated speech diarization experiments yield diarization error rates between  and  providing reference points for future improvements the results highlight the persistent difficulty of conversational asr particularly due to disfluencies overlaps and informal speech patterns by releasing these datasets and baselines we aim to advance hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages",
    "tokenized_abstract": [
      "advancement",
      "automatic",
      "speech",
      "recognition",
      "asr",
      "largely",
      "enhance",
      "extensive",
      "datasets",
      "highresource",
      "language",
      "language",
      "hungarian",
      "remain",
      "underrepresented",
      "due",
      "limited",
      "spontaneous",
      "conversational",
      "corpus",
      "address",
      "gap",
      "introduce",
      "two",
      "new",
      "datasets",
      "bealarge",
      "beadialogue",
      "construct",
      "previously",
      "unprocessed",
      "portion",
      "hungarian",
      "speech",
      "corpus",
      "name",
      "bea",
      "bealarge",
      "extend",
      "beabase",
      "hour",
      "spontaneous",
      "speech",
      "speaker",
      "enrich",
      "detailed",
      "segmentlevel",
      "metadata",
      "beadialogue",
      "comprise",
      "hour",
      "spontaneous",
      "conversation",
      "hungarian",
      "speech",
      "corpus",
      "feature",
      "natural",
      "dialogue",
      "partition",
      "speakerindependent",
      "subset",
      "support",
      "research",
      "conversational",
      "asr",
      "speaker",
      "diarization",
      "establish",
      "reproducible",
      "baseline",
      "datasets",
      "use",
      "publicly",
      "available",
      "asr",
      "model",
      "finetuned",
      "fast",
      "conformer",
      "model",
      "achieve",
      "word",
      "error",
      "rate",
      "low",
      "spontaneous",
      "repeat",
      "speech",
      "diarization",
      "experiment",
      "yield",
      "diarization",
      "error",
      "rate",
      "provide",
      "reference",
      "point",
      "future",
      "improvement",
      "result",
      "highlight",
      "persistent",
      "difficulty",
      "conversational",
      "asr",
      "particularly",
      "due",
      "disfluency",
      "overlap",
      "informal",
      "speech",
      "pattern",
      "release",
      "datasets",
      "baseline",
      "aim",
      "advance",
      "hungarian",
      "speech",
      "technology",
      "offer",
      "methodological",
      "framework",
      "develop",
      "spontaneous",
      "conversational",
      "benchmark",
      "language"
    ],
    "bm25_score": 9.666,
    "is_selected": 1
  },
  {
    "id": "2511.13612v1",
    "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
    "original_abstract": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
    "cleaned_abstract": "recent progress in large language models llms has moved the frontier from puzzlesolving to sciencegrade reasoningthe kind needed to tackle problems whose answers must stand against nature not merely fit a rubric physics is the sharpest test of this shift which binds symbols to reality in a fundamental way serving as the cornerstone of most modern technologies in this work we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities especially excel at solving olympiadlevel physics problems we introduce p a family of opensource physics reasoning models trained entirely through reinforcement learning rl among them pbab is the first opensource model with goldmedal performance at the latest international physics olympiad ipho  and wins  gold medals out of  internationalregional physics competitions in  pbab also surpasses almost all other opensource models on ipho  getting a silver medal further equipped with an agentic framework physicsminions pbabphysicsminions achieves overall no on ipho  and obtains the highest average score over the  physics competitions besides physics p models also present great performance on other reasoning tasks like math and coding showing the great generalibility of p series",
    "tokenized_abstract": [
      "recent",
      "progress",
      "large",
      "language",
      "model",
      "llms",
      "move",
      "frontier",
      "puzzlesolving",
      "sciencegrade",
      "reasoningthe",
      "kind",
      "need",
      "tackle",
      "problem",
      "whose",
      "answer",
      "must",
      "stand",
      "nature",
      "merely",
      "fit",
      "rubric",
      "physic",
      "sharp",
      "test",
      "shift",
      "bind",
      "symbol",
      "reality",
      "fundamental",
      "way",
      "serve",
      "cornerstone",
      "modern",
      "technology",
      "work",
      "manage",
      "advance",
      "physic",
      "research",
      "develop",
      "large",
      "language",
      "model",
      "exceptional",
      "physic",
      "reason",
      "capability",
      "especially",
      "excel",
      "solve",
      "olympiadlevel",
      "physic",
      "problem",
      "introduce",
      "family",
      "opensource",
      "physic",
      "reason",
      "model",
      "train",
      "entirely",
      "reinforcement",
      "learn",
      "rl",
      "among",
      "pbab",
      "first",
      "opensource",
      "model",
      "goldmedal",
      "performance",
      "late",
      "international",
      "physic",
      "olympiad",
      "ipho",
      "win",
      "gold",
      "medal",
      "internationalregional",
      "physic",
      "competition",
      "pbab",
      "also",
      "surpass",
      "almost",
      "opensource",
      "model",
      "ipho",
      "get",
      "silver",
      "medal",
      "equip",
      "agentic",
      "framework",
      "physicsminions",
      "pbabphysicsminions",
      "achieves",
      "overall",
      "ipho",
      "obtain",
      "high",
      "average",
      "score",
      "physic",
      "competition",
      "besides",
      "physic",
      "model",
      "also",
      "present",
      "great",
      "performance",
      "reason",
      "task",
      "like",
      "math",
      "cod",
      "show",
      "great",
      "generalibility",
      "series"
    ],
    "bm25_score": 6.9875,
    "is_selected": 1
  },
  {
    "id": "2511.13630v1",
    "title": "Beyond Mimicry: Preference Coherence in LLMs",
    "original_abstract": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.",
    "cleaned_abstract": "we investigate whether large language models exhibit genuine preference structures by testing their responses to aispecific tradeoffs involving gpu reduction capability restrictions shutdown deletion oversight and leisure time allocation analyzing eight stateoftheart models across  modelcategory combinations using logistic regression and behavioral classification we find that  combinations  demonstrated statistically significant relationships between scenario intensity and choice patterns with   exhibiting withinrange switching points however only  combinations  demonstrate meaningful preference coherence through adaptive or thresholdbased behavior while   show no detectable tradeoff behavior the observed patterns can be explained by three distinct decisionmaking architectures comprehensive tradeoff systems selective trigger mechanisms and no stable decisionmaking paradigm testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization the prevalence of unstable transitions  and stimulusspecific sensitivities suggests current ai systems lack unified preference structures raising concerns about deployment in contexts requiring complex value tradeoffs",
    "tokenized_abstract": [
      "investigate",
      "whether",
      "large",
      "language",
      "model",
      "exhibit",
      "genuine",
      "preference",
      "structure",
      "test",
      "response",
      "aispecific",
      "tradeoff",
      "involve",
      "gpu",
      "reduction",
      "capability",
      "restriction",
      "shutdown",
      "deletion",
      "oversight",
      "leisure",
      "time",
      "allocation",
      "analyze",
      "eight",
      "stateoftheart",
      "model",
      "across",
      "modelcategory",
      "combination",
      "use",
      "logistic",
      "regression",
      "behavioral",
      "classification",
      "find",
      "combination",
      "demonstrate",
      "statistically",
      "significant",
      "relationship",
      "scenario",
      "intensity",
      "choice",
      "pattern",
      "exhibit",
      "withinrange",
      "switch",
      "point",
      "however",
      "combination",
      "demonstrate",
      "meaningful",
      "preference",
      "coherence",
      "adaptive",
      "thresholdbased",
      "behavior",
      "show",
      "detectable",
      "tradeoff",
      "behavior",
      "observed",
      "pattern",
      "explain",
      "three",
      "distinct",
      "decisionmaking",
      "architecture",
      "comprehensive",
      "tradeoff",
      "system",
      "selective",
      "trigger",
      "mechanism",
      "stable",
      "decisionmaking",
      "paradigm",
      "test",
      "instrumental",
      "hypothesis",
      "temporal",
      "horizon",
      "manipulation",
      "reveals",
      "paradoxical",
      "pattern",
      "inconsistent",
      "pure",
      "strategic",
      "optimization",
      "prevalence",
      "unstable",
      "transition",
      "stimulusspecific",
      "sensitivity",
      "suggest",
      "current",
      "ai",
      "system",
      "lack",
      "unified",
      "preference",
      "structure",
      "raise",
      "concern",
      "deployment",
      "contexts",
      "require",
      "complex",
      "value",
      "tradeoff"
    ],
    "bm25_score": 5.3975,
    "is_selected": 1
  },
  {
    "id": "2511.13575v1",
    "title": "Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification",
    "original_abstract": "Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.",
    "cleaned_abstract": "person reidentification reid aims to retrieve target pedestrian images given either visual queries imagetoimage ii or textual descriptions texttoimage ti although both tasks share a common retrieval objective they pose distinct challenges ii emphasizes discriminative identity learning while ti requires accurate crossmodal semantic alignment existing methods often treat these tasks separately which may lead to representation entanglement and suboptimal performance to address this we propose a unified framework named hierarchical prompt learning hpl which leverages taskaware prompt modeling to jointly optimize both tasks specifically we first introduce a taskrouted transformer which incorporates dual classification tokens into a shared visual encoder to route features for ii and ti branches respectively on top of this we develop a hierarchical prompt generation scheme that integrates identitylevel learnable tokens with instancelevel pseudotext tokens these pseudotokens are derived from image or text features via modalityspecific inversion networks injecting finegrained instancespecific semantics into the prompts furthermore we propose a crossmodal prompt regularization strategy to enforce semantic alignment in the prompt token space ensuring that pseudoprompts preserve sourcemodality characteristics while enhancing crossmodal transferability extensive experiments on multiple reid benchmarks validate the effectiveness of our method achieving stateoftheart performance on both ii and ti tasks",
    "tokenized_abstract": [
      "person",
      "reidentification",
      "reid",
      "aim",
      "retrieve",
      "target",
      "pedestrian",
      "image",
      "give",
      "either",
      "visual",
      "query",
      "imagetoimage",
      "ii",
      "textual",
      "description",
      "texttoimage",
      "ti",
      "although",
      "task",
      "share",
      "common",
      "retrieval",
      "objective",
      "pose",
      "distinct",
      "challenge",
      "ii",
      "emphasize",
      "discriminative",
      "identity",
      "learn",
      "ti",
      "require",
      "accurate",
      "crossmodal",
      "semantic",
      "alignment",
      "exist",
      "method",
      "often",
      "treat",
      "task",
      "separately",
      "may",
      "lead",
      "representation",
      "entanglement",
      "suboptimal",
      "performance",
      "address",
      "propose",
      "unified",
      "framework",
      "name",
      "hierarchical",
      "prompt",
      "learn",
      "hpl",
      "leverage",
      "taskaware",
      "prompt",
      "modeling",
      "jointly",
      "optimize",
      "task",
      "specifically",
      "first",
      "introduce",
      "taskrouted",
      "transformer",
      "incorporate",
      "dual",
      "classification",
      "tokens",
      "share",
      "visual",
      "encoder",
      "route",
      "feature",
      "ii",
      "ti",
      "branch",
      "respectively",
      "top",
      "develop",
      "hierarchical",
      "prompt",
      "generation",
      "scheme",
      "integrate",
      "identitylevel",
      "learnable",
      "token",
      "instancelevel",
      "pseudotext",
      "tokens",
      "pseudotokens",
      "derive",
      "image",
      "text",
      "feature",
      "via",
      "modalityspecific",
      "inversion",
      "network",
      "inject",
      "finegrained",
      "instancespecific",
      "semantics",
      "prompt",
      "furthermore",
      "propose",
      "crossmodal",
      "prompt",
      "regularization",
      "strategy",
      "enforce",
      "semantic",
      "alignment",
      "prompt",
      "token",
      "space",
      "ensure",
      "pseudoprompts",
      "preserve",
      "sourcemodality",
      "characteristic",
      "enhance",
      "crossmodal",
      "transferability",
      "extensive",
      "experiment",
      "multiple",
      "reid",
      "benchmark",
      "validate",
      "effectiveness",
      "method",
      "achieve",
      "stateoftheart",
      "performance",
      "ii",
      "ti",
      "task"
    ],
    "bm25_score": 5.3944,
    "is_selected": 1
  },
  {
    "id": "2511.13590v1",
    "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation",
    "original_abstract": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.",
    "cleaned_abstract": "texttosql datasets are essential for training and evaluating texttosql models but existing datasets often suffer from limited coverage and fail to capture the diversity of realworld applications to address this we propose a novel taxonomy for texttosql classification based on dimensions including core intents statement types syntax structures and key actions using this taxonomy we evaluate widely used public texttosql datasets eg spider and bird and reveal limitations in their coverage and diversity we then introduce a taxonomyguided dataset synthesis pipeline yielding a new dataset named sqlsynth this approach combines the taxonomy with large language models llms to ensure the dataset reflects the breadth and complexity of realworld texttosql applications extensive analysis and experimental results validate the effectiveness of our taxonomy as sqlsynth exhibits greater diversity and coverage compared to existing benchmarks moreover we uncover that existing llms typically fall short in adequately capturing the full range of scenarios resulting in limited performance on sqlsynth however finetuning can substantially improve their performance in these scenarios the proposed taxonomy has significant potential impact as it not only enables comprehensive analysis of datasets and the performance of different llms but also guides the construction of training data for llms",
    "tokenized_abstract": [
      "texttosql",
      "datasets",
      "essential",
      "training",
      "evaluate",
      "texttosql",
      "model",
      "exist",
      "datasets",
      "often",
      "suffer",
      "limited",
      "coverage",
      "fail",
      "capture",
      "diversity",
      "realworld",
      "application",
      "address",
      "propose",
      "novel",
      "taxonomy",
      "texttosql",
      "classification",
      "base",
      "dimension",
      "include",
      "core",
      "intent",
      "statement",
      "type",
      "syntax",
      "structure",
      "key",
      "action",
      "use",
      "taxonomy",
      "evaluate",
      "widely",
      "use",
      "public",
      "texttosql",
      "datasets",
      "eg",
      "spider",
      "bird",
      "reveal",
      "limitation",
      "coverage",
      "diversity",
      "introduce",
      "taxonomyguided",
      "dataset",
      "synthesis",
      "pipeline",
      "yield",
      "new",
      "dataset",
      "name",
      "sqlsynth",
      "approach",
      "combine",
      "taxonomy",
      "large",
      "language",
      "model",
      "llms",
      "ensure",
      "dataset",
      "reflect",
      "breadth",
      "complexity",
      "realworld",
      "texttosql",
      "application",
      "extensive",
      "analysis",
      "experimental",
      "result",
      "validate",
      "effectiveness",
      "taxonomy",
      "sqlsynth",
      "exhibit",
      "great",
      "diversity",
      "coverage",
      "compare",
      "exist",
      "benchmark",
      "moreover",
      "uncover",
      "exist",
      "llm",
      "typically",
      "fall",
      "short",
      "adequately",
      "capture",
      "full",
      "range",
      "scenario",
      "result",
      "limited",
      "performance",
      "sqlsynth",
      "however",
      "finetuning",
      "substantially",
      "improve",
      "performance",
      "scenario",
      "propose",
      "taxonomy",
      "significant",
      "potential",
      "impact",
      "enable",
      "comprehensive",
      "analysis",
      "datasets",
      "performance",
      "different",
      "llm",
      "also",
      "guide",
      "construction",
      "training",
      "data",
      "llm"
    ],
    "bm25_score": 4.6002,
    "is_selected": 1
  },
  {
    "id": "2511.13626v1",
    "title": "CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product",
    "original_abstract": "Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.",
    "cleaned_abstract": "humandefined creativity is highly abstract posing a challenge for multimodal large language models mllms to comprehend and assess creativity that aligns with human judgments the absence of an existing benchmark further exacerbates this dilemma to this end we propose crebench which consists of two key components  an evaluation benchmark covering the multiple dimensions from creative idea to process to products  cremit creativity multimodal instruction tuning dataset a multimodal creativity evaluation dataset consisting of k diversesourced multimodal data k human feedbacks and m multityped instructions specifically to ensure mllms can handle diverse creativityrelated queries we prompt gpt to refine these human feedbacks to activate stronger creativity assessment capabilities crebench serves as a foundation for building mllms that understand humanaligned creativity based on the crebench we finetune opensource general mllms resulting in creexpert a multimodal creativity evaluation expert model extensive experiments demonstrate that the proposed creexpert models achieve significantly better alignment with human creativity evaluation compared to stateoftheart mllms including the most advanced gptv and geminiprovision",
    "tokenized_abstract": [
      "humandefined",
      "creativity",
      "highly",
      "abstract",
      "pose",
      "challenge",
      "multimodal",
      "large",
      "language",
      "model",
      "mllms",
      "comprehend",
      "assess",
      "creativity",
      "align",
      "human",
      "judgment",
      "absence",
      "exist",
      "benchmark",
      "exacerbate",
      "dilemma",
      "end",
      "propose",
      "crebench",
      "consist",
      "two",
      "key",
      "component",
      "evaluation",
      "benchmark",
      "cover",
      "multiple",
      "dimension",
      "creative",
      "idea",
      "process",
      "product",
      "cremit",
      "creativity",
      "multimodal",
      "instruction",
      "tune",
      "dataset",
      "multimodal",
      "creativity",
      "evaluation",
      "dataset",
      "consisting",
      "diversesourced",
      "multimodal",
      "data",
      "human",
      "feedback",
      "multityped",
      "instruction",
      "specifically",
      "ensure",
      "mllms",
      "handle",
      "diverse",
      "creativityrelated",
      "query",
      "prompt",
      "gpt",
      "refine",
      "human",
      "feedback",
      "activate",
      "strong",
      "creativity",
      "assessment",
      "capability",
      "crebench",
      "serf",
      "foundation",
      "build",
      "mllms",
      "understand",
      "humanaligned",
      "creativity",
      "base",
      "crebench",
      "finetune",
      "opensource",
      "general",
      "mllms",
      "result",
      "creexpert",
      "multimodal",
      "creativity",
      "evaluation",
      "expert",
      "model",
      "extensive",
      "experiment",
      "demonstrate",
      "propose",
      "creexpert",
      "model",
      "achieve",
      "significantly",
      "well",
      "alignment",
      "human",
      "creativity",
      "evaluation",
      "compare",
      "stateoftheart",
      "mllms",
      "include",
      "advanced",
      "gptv",
      "geminiprovision"
    ],
    "bm25_score": 4.4316,
    "is_selected": 1
  },
  {
    "id": "2511.13640v1",
    "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures",
    "original_abstract": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.",
    "cleaned_abstract": "the rapid progress of large language models llms is fueled by the growing reliance on datasets that blend real and synthetic data while synthetic data offers scalability and costefficiency it often introduces systematic distributional discrepancies particularly underrepresenting longtail knowledge due to truncation effects from data generation mechanisms like topp sampling temperature scaling and finite sampling these discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed realsynthetic datasets in this paper we identify a threephase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge we further derive an llm generalization bound designed for real and synthetic mixtures revealing several key factors that govern their generalization performance building on our theoretical findings we propose an effective yet efficient data valuation method that scales to largescale datasets comprehensive experiments across four tasks including image classification sentiment classification instruction following and complex reasoning demonstrate that our method surpasses stateoftheart baselines in data valuation with significantly low computational cost",
    "tokenized_abstract": [
      "rapid",
      "progress",
      "large",
      "language",
      "model",
      "llms",
      "fuel",
      "grow",
      "reliance",
      "datasets",
      "blend",
      "real",
      "synthetic",
      "data",
      "synthetic",
      "data",
      "offer",
      "scalability",
      "costefficiency",
      "often",
      "introduce",
      "systematic",
      "distributional",
      "discrepancy",
      "particularly",
      "underrepresenting",
      "longtail",
      "knowledge",
      "due",
      "truncation",
      "effect",
      "data",
      "generation",
      "mechanism",
      "like",
      "topp",
      "sample",
      "temperature",
      "scaling",
      "finite",
      "sample",
      "discrepancy",
      "pose",
      "fundamental",
      "challenge",
      "characterize",
      "evaluate",
      "utility",
      "mixed",
      "realsynthetic",
      "datasets",
      "paper",
      "identify",
      "threephase",
      "scale",
      "behavior",
      "characterize",
      "two",
      "breakpoints",
      "reflect",
      "transition",
      "model",
      "behavior",
      "across",
      "learn",
      "head",
      "tail",
      "knowledge",
      "derive",
      "llm",
      "generalization",
      "bound",
      "design",
      "real",
      "synthetic",
      "mixture",
      "reveal",
      "several",
      "key",
      "factor",
      "govern",
      "generalization",
      "performance",
      "building",
      "theoretical",
      "finding",
      "propose",
      "effective",
      "yet",
      "efficient",
      "data",
      "valuation",
      "method",
      "scale",
      "largescale",
      "datasets",
      "comprehensive",
      "experiment",
      "across",
      "four",
      "task",
      "include",
      "image",
      "classification",
      "sentiment",
      "classification",
      "instruction",
      "follow",
      "complex",
      "reasoning",
      "demonstrate",
      "method",
      "surpass",
      "stateoftheart",
      "baseline",
      "data",
      "valuation",
      "significantly",
      "low",
      "computational",
      "cost"
    ],
    "bm25_score": 4.3391,
    "is_selected": 1
  },
  {
    "id": "2511.13565v1",
    "title": "Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction",
    "original_abstract": "Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.",
    "cleaned_abstract": "intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing humanmachine interaction traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques to overcome these issues we introduce the concept of humansymbiotic health intelligence hshi which is a framework that integrates multimodal sensor networks with edgecloud collaborative computing and a hybrid approach to data and knowledge modeling hshi is designed to adapt dynamically to both interindividual and intraindividual variability transitioning health management from passive monitoring to an active collaborative evolution the framework incorporates aidriven optimization of materials and microstructures provides robust interpretation of multimodal signals and utilizes a dual mechanism that merges populationlevel insights with personalized adaptations moreover the integration of closedloop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback in general hshi represents a significant shift in healthcare moving towards a model that emphasizes prevention adaptability and a harmonious relationship between technology and health management",
    "tokenized_abstract": [
      "intelligent",
      "wearable",
      "system",
      "forefront",
      "precision",
      "medicine",
      "play",
      "crucial",
      "role",
      "enhance",
      "humanmachine",
      "interaction",
      "traditional",
      "device",
      "often",
      "encounter",
      "limitation",
      "due",
      "dependence",
      "empirical",
      "material",
      "design",
      "basic",
      "signal",
      "processing",
      "technique",
      "overcome",
      "issue",
      "introduce",
      "concept",
      "humansymbiotic",
      "health",
      "intelligence",
      "hshi",
      "framework",
      "integrate",
      "multimodal",
      "sensor",
      "network",
      "edgecloud",
      "collaborative",
      "computing",
      "hybrid",
      "approach",
      "data",
      "knowledge",
      "modeling",
      "hshi",
      "design",
      "adapt",
      "dynamically",
      "interindividual",
      "intraindividual",
      "variability",
      "transition",
      "health",
      "management",
      "passive",
      "monitoring",
      "active",
      "collaborative",
      "evolution",
      "framework",
      "incorporate",
      "aidriven",
      "optimization",
      "material",
      "microstructures",
      "provide",
      "robust",
      "interpretation",
      "multimodal",
      "signal",
      "utilize",
      "dual",
      "mechanism",
      "merge",
      "populationlevel",
      "insight",
      "personalized",
      "adaptation",
      "moreover",
      "integration",
      "closedloop",
      "optimization",
      "reinforcement",
      "learning",
      "digital",
      "twin",
      "facilitates",
      "customize",
      "intervention",
      "feedback",
      "general",
      "hshi",
      "represent",
      "significant",
      "shift",
      "healthcare",
      "move",
      "towards",
      "model",
      "emphasize",
      "prevention",
      "adaptability",
      "harmonious",
      "relationship",
      "technology",
      "health",
      "management"
    ],
    "bm25_score": 4.0419,
    "is_selected": 1
  },
  {
    "id": "2511.13710v1",
    "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands",
    "original_abstract": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision",
    "cleaned_abstract": "human grasps can be roughly categorized into two types power grasps and precision grasps precision grasping enables tool use and is believed to have influenced human evolution todays multifingered robotic hands are effective in power grasps but for tasks requiring precision parallel grippers are still more widely adopted this contrast highlights a key limitation in current robotic hand design the difficulty of achieving both stable power grasps and precise finegrained manipulation within a single versatile system in this work we bridge this gap by jointly optimizing the control and hardware design of a multifingered dexterous hand enabling both power and precision manipulation rather than redesigning the entire hand we introduce a lightweight fingertip geometry modification represent it as a contact plane and jointly optimize its parameters along with the corresponding control our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumbindex motions which proves robust for simtoreal transfer on the design side we leverage largescale simulation to optimize the fingertip geometry using a differentiable neuralphysics surrogate model we validate our approach through extensive experiments in both simtoreal and realtoreal settings our method achieves an  zeroshot success rate on unseen objects in simtoreal precision grasping and a  success rate in challenging realworld tasks involving bread pinching these results demonstrate that our codesign framework can significantly enhance the finegrained manipulation ability of multifingered hands without reducing their ability for power grasps our project page is at httpsjianglongyecompowertoprecision",
    "tokenized_abstract": [
      "human",
      "grasp",
      "roughly",
      "categorize",
      "two",
      "type",
      "power",
      "grasp",
      "precision",
      "grasp",
      "precision",
      "grasp",
      "enables",
      "tool",
      "use",
      "believe",
      "influence",
      "human",
      "evolution",
      "today",
      "multifingered",
      "robotic",
      "hand",
      "effective",
      "power",
      "grasp",
      "task",
      "require",
      "precision",
      "parallel",
      "grippers",
      "still",
      "widely",
      "adopt",
      "contrast",
      "highlight",
      "key",
      "limitation",
      "current",
      "robotic",
      "hand",
      "design",
      "difficulty",
      "achieve",
      "stable",
      "power",
      "grasp",
      "precise",
      "finegrained",
      "manipulation",
      "within",
      "single",
      "versatile",
      "system",
      "work",
      "bridge",
      "gap",
      "jointly",
      "optimize",
      "control",
      "hardware",
      "design",
      "multifingered",
      "dexterous",
      "hand",
      "enable",
      "power",
      "precision",
      "manipulation",
      "rather",
      "redesign",
      "entire",
      "hand",
      "introduce",
      "lightweight",
      "fingertip",
      "geometry",
      "modification",
      "represent",
      "contact",
      "plane",
      "jointly",
      "optimize",
      "parameter",
      "along",
      "correspond",
      "control",
      "control",
      "strategy",
      "dynamically",
      "switch",
      "power",
      "precision",
      "manipulation",
      "simplifies",
      "precision",
      "control",
      "parallel",
      "thumbindex",
      "motion",
      "prove",
      "robust",
      "simtoreal",
      "transfer",
      "design",
      "side",
      "leverage",
      "largescale",
      "simulation",
      "optimize",
      "fingertip",
      "geometry",
      "use",
      "differentiable",
      "neuralphysics",
      "surrogate",
      "model",
      "validate",
      "approach",
      "extensive",
      "experiment",
      "simtoreal",
      "realtoreal",
      "setting",
      "method",
      "achieve",
      "zeroshot",
      "success",
      "rate",
      "unseen",
      "object",
      "simtoreal",
      "precision",
      "grasping",
      "success",
      "rate",
      "challenge",
      "realworld",
      "task",
      "involve",
      "bread",
      "pinch",
      "result",
      "demonstrate",
      "codesign",
      "framework",
      "significantly",
      "enhance",
      "finegrained",
      "manipulation",
      "ability",
      "multifingered",
      "hand",
      "without",
      "reduce",
      "ability",
      "power",
      "grasp",
      "project",
      "page",
      "httpsjianglongyecompowertoprecision"
    ],
    "bm25_score": 3.422,
    "is_selected": 1
  },
  {
    "id": "2511.13526v1",
    "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models",
    "original_abstract": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.",
    "cleaned_abstract": "artificial intelligence ai is reshaping modern healthcare by advancing disease diagnosis treatment decisionmaking and biomedical research among ai technologies large language models llms have become especially impactful enabling deep knowledge extraction and semantic reasoning from complex medical texts however effective clinical decision support requires knowledge in structured interoperable formats knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks yet current clinical knowledge graphs still depend heavily on manual curation and rulebased extraction which is limited by the complexity and contextual ambiguity of medical guidelines and literature to overcome these challenges we propose an automated framework that combines retrievalaugmented generation rag with llms to construct medical indicator knowledge graphs the framework incorporates guidelinedriven data acquisition ontologybased schema design and expertintheloop validation to ensure scalability accuracy and clinical reliability the resulting knowledge graphs can be integrated into intelligent diagnosis and questionanswering systems accelerating the development of aidriven healthcare solutions",
    "tokenized_abstract": [
      "artificial",
      "intelligence",
      "ai",
      "reshape",
      "modern",
      "healthcare",
      "advance",
      "disease",
      "diagnosis",
      "treatment",
      "decisionmaking",
      "biomedical",
      "research",
      "among",
      "ai",
      "technology",
      "large",
      "language",
      "model",
      "llms",
      "become",
      "especially",
      "impactful",
      "enable",
      "deep",
      "knowledge",
      "extraction",
      "semantic",
      "reasoning",
      "complex",
      "medical",
      "text",
      "however",
      "effective",
      "clinical",
      "decision",
      "support",
      "require",
      "knowledge",
      "structured",
      "interoperable",
      "format",
      "knowledge",
      "graphs",
      "serve",
      "role",
      "integrate",
      "heterogeneous",
      "medical",
      "information",
      "semantically",
      "consistent",
      "network",
      "yet",
      "current",
      "clinical",
      "knowledge",
      "graph",
      "still",
      "depend",
      "heavily",
      "manual",
      "curation",
      "rulebased",
      "extraction",
      "limit",
      "complexity",
      "contextual",
      "ambiguity",
      "medical",
      "guideline",
      "literature",
      "overcome",
      "challenge",
      "propose",
      "automated",
      "framework",
      "combine",
      "retrievalaugmented",
      "generation",
      "rag",
      "llm",
      "construct",
      "medical",
      "indicator",
      "knowledge",
      "graph",
      "framework",
      "incorporate",
      "guidelinedriven",
      "data",
      "acquisition",
      "ontologybased",
      "schema",
      "design",
      "expertintheloop",
      "validation",
      "ensure",
      "scalability",
      "accuracy",
      "clinical",
      "reliability",
      "result",
      "knowledge",
      "graph",
      "integrate",
      "intelligent",
      "diagnosis",
      "questionanswering",
      "system",
      "accelerate",
      "development",
      "aidriven",
      "healthcare",
      "solution"
    ],
    "bm25_score": 3.2995,
    "is_selected": 1
  },
  {
    "id": "2511.13548v1",
    "title": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models",
    "original_abstract": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across character, word, and sentence-level operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.",
    "cleaned_abstract": "the rapid adoption of large language models llms has brought both transformative applications and new security risks including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs existing automated jailbreak generation approaches eg autodan suffer from limited mutation diversity shallow fitness evaluation and fragile keywordbased detection to address these limitations we propose forgedan a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned llms first forgedan introduces multistrategy textual perturbations across character word and sentencelevel operations to enhance attack diversity then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs finally forgedan integrates dualdimensional jailbreak judgment leveraging an llmbased classifier to jointly assess model compliance and output harmfulness thereby reducing false positives and improving detection effectiveness our evaluation demonstrates forgedan achieves high jailbreaking success rates while maintaining naturalness and stealth outperforming existing sota solutions",
    "tokenized_abstract": [
      "rapid",
      "adoption",
      "large",
      "language",
      "model",
      "llms",
      "bring",
      "transformative",
      "application",
      "new",
      "security",
      "risk",
      "include",
      "jailbreak",
      "attack",
      "bypass",
      "alignment",
      "safeguard",
      "elicit",
      "harmful",
      "output",
      "exist",
      "automate",
      "jailbreak",
      "generation",
      "approach",
      "eg",
      "autodan",
      "suffer",
      "limited",
      "mutation",
      "diversity",
      "shallow",
      "fitness",
      "evaluation",
      "fragile",
      "keywordbased",
      "detection",
      "address",
      "limitation",
      "propose",
      "forgedan",
      "novel",
      "evolutionary",
      "framework",
      "generate",
      "semantically",
      "coherent",
      "highly",
      "effective",
      "adversarial",
      "prompt",
      "align",
      "llm",
      "first",
      "forgedan",
      "introduces",
      "multistrategy",
      "textual",
      "perturbation",
      "across",
      "character",
      "word",
      "sentencelevel",
      "operation",
      "enhance",
      "attack",
      "diversity",
      "employ",
      "interpretable",
      "semantic",
      "fitness",
      "evaluation",
      "base",
      "text",
      "similarity",
      "model",
      "guide",
      "evolutionary",
      "process",
      "toward",
      "semantically",
      "relevant",
      "harmful",
      "output",
      "finally",
      "forgedan",
      "integrates",
      "dualdimensional",
      "jailbreak",
      "judgment",
      "leverage",
      "llmbased",
      "classifier",
      "jointly",
      "assess",
      "model",
      "compliance",
      "output",
      "harmfulness",
      "thereby",
      "reduce",
      "false",
      "positive",
      "improve",
      "detection",
      "effectiveness",
      "evaluation",
      "demonstrate",
      "forgedan",
      "achieves",
      "high",
      "jailbreaking",
      "success",
      "rate",
      "maintain",
      "naturalness",
      "stealth",
      "outperform",
      "exist",
      "sota",
      "solution"
    ],
    "bm25_score": 3.0879,
    "is_selected": 1
  },
  {
    "id": "2511.13712v1",
    "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness",
    "original_abstract": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.",
    "cleaned_abstract": "as climate change accelerates the frequency and severity of extreme events such as wildfires the need for accurate explainable and actionable forecasting becomes increasingly urgent while artificial intelligence ai models have shown promise in predicting such events their adoption in realworld decisionmaking remains limited due to their blackbox nature which limits trust explainability and operational readiness this paper investigates the role of explainable ai xai in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting using wildfire prediction as a case study we evaluate various ai models and employ shapley additive explanations shap to uncover key features decision pathways and potential biases in model behavior our analysis demonstrates how xai not only clarifies model reasoning but also supports critical decisionmaking by domain experts and response teams in addition we provide supporting visualizations that enhance the interpretability of xai outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics this approach enhances the usability of ai explanations for practitioners and policymakers our findings highlight the need for ai systems that are not only accurate but also interpretable accessible and trustworthy essential for effective use in disaster preparedness risk mitigation and climate resilience planning",
    "tokenized_abstract": [
      "climate",
      "change",
      "accelerate",
      "frequency",
      "severity",
      "extreme",
      "event",
      "wildfire",
      "need",
      "accurate",
      "explainable",
      "actionable",
      "forecasting",
      "become",
      "increasingly",
      "urgent",
      "artificial",
      "intelligence",
      "ai",
      "model",
      "show",
      "promise",
      "predict",
      "event",
      "adoption",
      "realworld",
      "decisionmaking",
      "remain",
      "limited",
      "due",
      "blackbox",
      "nature",
      "limit",
      "trust",
      "explainability",
      "operational",
      "readiness",
      "paper",
      "investigate",
      "role",
      "explainable",
      "ai",
      "xai",
      "bridge",
      "gap",
      "predictive",
      "accuracy",
      "actionable",
      "insight",
      "extreme",
      "event",
      "forecast",
      "use",
      "wildfire",
      "prediction",
      "case",
      "study",
      "evaluate",
      "various",
      "ai",
      "model",
      "employ",
      "shapley",
      "additive",
      "explanation",
      "shap",
      "uncover",
      "key",
      "feature",
      "decision",
      "pathway",
      "potential",
      "bias",
      "model",
      "behavior",
      "analysis",
      "demonstrate",
      "xai",
      "clarify",
      "model",
      "reason",
      "also",
      "support",
      "critical",
      "decisionmaking",
      "domain",
      "expert",
      "response",
      "team",
      "addition",
      "provide",
      "support",
      "visualization",
      "enhance",
      "interpretability",
      "xai",
      "output",
      "contextualizing",
      "feature",
      "importance",
      "temporal",
      "pattern",
      "seasonality",
      "geospatial",
      "characteristic",
      "approach",
      "enhance",
      "usability",
      "ai",
      "explanation",
      "practitioner",
      "policymakers",
      "finding",
      "highlight",
      "need",
      "ai",
      "system",
      "accurate",
      "also",
      "interpretable",
      "accessible",
      "trustworthy",
      "essential",
      "effective",
      "use",
      "disaster",
      "preparedness",
      "risk",
      "mitigation",
      "climate",
      "resilience",
      "planning"
    ],
    "bm25_score": 2.5126,
    "is_selected": 1
  },
  {
    "id": "2511.13542v1",
    "title": "Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop",
    "original_abstract": "Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.",
    "cleaned_abstract": "adaptive learning often diagnoses precisely yet intervenes weakly producing help that is mistimed or misaligned this study presents evidence supporting an instructorgoverned feedback loop that converts conceptlevel assessment evidence into vetted microinterventions the adaptive learning algorithm includes three safeguards adequacy as a hard guarantee of gap closure attention as a budgeted limit for time and redundancy and diversity as protection against overfitting to a single resource we formulate intervention assignment as a binary integer program with constraints for coverage time difficulty windows derived from ability estimates prerequisites encoded by a concept matrix and antiredundancy with diversity greedy selection serves lowrichness and tightlatency settings gradientbased relaxation serves rich repositories and a hybrid switches along a richnesslatency frontier in simulation and in an introductory physics deployment with  students both solvers achieved full skill coverage for nearly all learners within bounded watch time the gradientbased method reduced redundant coverage by about  percentage points relative to greedy and produced more consistent difficulty alignment while greedy delivered comparable adequacy at lower computational cost in resourcescarce environments slack variables localized missing content and guided targeted curation sustaining sufficiency across student subgroups the result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable loadaware personalization at the classroom scale",
    "tokenized_abstract": [
      "adaptive",
      "learning",
      "often",
      "diagnose",
      "precisely",
      "yet",
      "intervenes",
      "weakly",
      "produce",
      "help",
      "mistime",
      "misalign",
      "study",
      "present",
      "evidence",
      "support",
      "instructorgoverned",
      "feedback",
      "loop",
      "convert",
      "conceptlevel",
      "assessment",
      "evidence",
      "vetted",
      "microinterventions",
      "adaptive",
      "learning",
      "algorithm",
      "include",
      "three",
      "safeguard",
      "adequacy",
      "hard",
      "guarantee",
      "gap",
      "closure",
      "attention",
      "budgeted",
      "limit",
      "time",
      "redundancy",
      "diversity",
      "protection",
      "overfitting",
      "single",
      "resource",
      "formulate",
      "intervention",
      "assignment",
      "binary",
      "integer",
      "program",
      "constraint",
      "coverage",
      "time",
      "difficulty",
      "window",
      "derive",
      "ability",
      "estimate",
      "prerequisite",
      "encode",
      "concept",
      "matrix",
      "antiredundancy",
      "diversity",
      "greedy",
      "selection",
      "serve",
      "lowrichness",
      "tightlatency",
      "setting",
      "gradientbased",
      "relaxation",
      "serf",
      "rich",
      "repository",
      "hybrid",
      "switch",
      "along",
      "richnesslatency",
      "frontier",
      "simulation",
      "introductory",
      "physic",
      "deployment",
      "student",
      "solver",
      "achieve",
      "full",
      "skill",
      "coverage",
      "nearly",
      "learner",
      "within",
      "bound",
      "watch",
      "time",
      "gradientbased",
      "method",
      "reduce",
      "redundant",
      "coverage",
      "percentage",
      "point",
      "relative",
      "greedy",
      "produce",
      "consistent",
      "difficulty",
      "alignment",
      "greedy",
      "deliver",
      "comparable",
      "adequacy",
      "low",
      "computational",
      "cost",
      "resourcescarce",
      "environment",
      "slack",
      "variable",
      "localize",
      "miss",
      "content",
      "guide",
      "target",
      "curation",
      "sustain",
      "sufficiency",
      "across",
      "student",
      "subgroups",
      "result",
      "tractable",
      "auditable",
      "controller",
      "close",
      "diagnostic",
      "pedagogical",
      "loop",
      "enable",
      "equitable",
      "loadaware",
      "personalization",
      "classroom",
      "scale"
    ],
    "bm25_score": 2.4133,
    "is_selected": 1
  },
  {
    "id": "2511.13670v1",
    "title": "Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process",
    "original_abstract": "This article develops the concept of Person-AI bidirectional fit, defined as the continuously evolving, context-sensitive alignment-primarily cognitive, but also emotional and behavioral-between a human decision-maker and an artificial intelligence system. Grounded in contingency theory and quality theory, the study examines the role of P-AI fit in managerial decision-making through a proof-of-concept case study involving a real hiring process for a Senior AI Lead. Three decision pathways are compared: (1) independent evaluations by a CEO, CTO, and CSO; (2) an evaluation produced by an augmented human-AI symbiotic intelligence system (H3LIX-LAIZA); and (3) an assessment generated by a general-purpose large language model. The results reveal substantial role-based divergence in human judgments, high alignment between H3LIX-LAIZA and the CEOs implicit decision model-including ethical disqualification of a high-risk candidate and a critical false-positive recommendation from the LLMr. The findings demonstrate that higher P-AI fit, exemplified by the CEO H3LIX-LAIZA relationship, functions as a mechanism linking augmented symbiotic intelligence to accurate, trustworthy, and context-sensitive decisions. The study provides an initial verification of the P-AI fit construct and a proof-of-concept for H3LIX-LAIZA as an augmented human-AI symbiotic intelligence system.",
    "cleaned_abstract": "this article develops the concept of personai bidirectional fit defined as the continuously evolving contextsensitive alignmentprimarily cognitive but also emotional and behavioralbetween a human decisionmaker and an artificial intelligence system grounded in contingency theory and quality theory the study examines the role of pai fit in managerial decisionmaking through a proofofconcept case study involving a real hiring process for a senior ai lead three decision pathways are compared  independent evaluations by a ceo cto and cso  an evaluation produced by an augmented humanai symbiotic intelligence system hlixlaiza and  an assessment generated by a generalpurpose large language model the results reveal substantial rolebased divergence in human judgments high alignment between hlixlaiza and the ceos implicit decision modelincluding ethical disqualification of a highrisk candidate and a critical falsepositive recommendation from the llmr the findings demonstrate that higher pai fit exemplified by the ceo hlixlaiza relationship functions as a mechanism linking augmented symbiotic intelligence to accurate trustworthy and contextsensitive decisions the study provides an initial verification of the pai fit construct and a proofofconcept for hlixlaiza as an augmented humanai symbiotic intelligence system",
    "tokenized_abstract": [
      "article",
      "develop",
      "concept",
      "personai",
      "bidirectional",
      "fit",
      "define",
      "continuously",
      "evolve",
      "contextsensitive",
      "alignmentprimarily",
      "cognitive",
      "also",
      "emotional",
      "behavioralbetween",
      "human",
      "decisionmaker",
      "artificial",
      "intelligence",
      "system",
      "ground",
      "contingency",
      "theory",
      "quality",
      "theory",
      "study",
      "examine",
      "role",
      "pai",
      "fit",
      "managerial",
      "decisionmaking",
      "proofofconcept",
      "case",
      "study",
      "involve",
      "real",
      "hiring",
      "process",
      "senior",
      "ai",
      "lead",
      "three",
      "decision",
      "pathway",
      "compare",
      "independent",
      "evaluation",
      "ceo",
      "cto",
      "cso",
      "evaluation",
      "produce",
      "augmented",
      "humanai",
      "symbiotic",
      "intelligence",
      "system",
      "hlixlaiza",
      "assessment",
      "generate",
      "generalpurpose",
      "large",
      "language",
      "model",
      "result",
      "reveal",
      "substantial",
      "rolebased",
      "divergence",
      "human",
      "judgment",
      "high",
      "alignment",
      "hlixlaiza",
      "ceo",
      "implicit",
      "decision",
      "modelincluding",
      "ethical",
      "disqualification",
      "highrisk",
      "candidate",
      "critical",
      "falsepositive",
      "recommendation",
      "llmr",
      "finding",
      "demonstrate",
      "high",
      "pai",
      "fit",
      "exemplify",
      "ceo",
      "hlixlaiza",
      "relationship",
      "function",
      "mechanism",
      "link",
      "augment",
      "symbiotic",
      "intelligence",
      "accurate",
      "trustworthy",
      "contextsensitive",
      "decision",
      "study",
      "provide",
      "initial",
      "verification",
      "pai",
      "fit",
      "construct",
      "proofofconcept",
      "hlixlaiza",
      "augmented",
      "humanai",
      "symbiotic",
      "intelligence",
      "system"
    ],
    "bm25_score": 2.0753,
    "is_selected": 1
  },
  {
    "id": "2511.13646v1",
    "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
    "original_abstract": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gdel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
    "cleaned_abstract": "large language models llms are reshaping almost all industries including software engineering in recent years a number of llm agents have been proposed to solve realworld software problems such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve endtoend software tasks while promising they typically require dedicated design and may still be suboptimal since it can be extremely challenging and costly to exhaust the entire agent scaffold design space recognizing that software agents are inherently software themselves that can be further refinedmodified researchers have proposed a number of selfimproving software agents recently including the darwingdel machine dgm meanwhile such selfimproving agents require costly offline training on specific benchmarks and may not generalize well across different llms or benchmarks in this paper we propose livesweagent the first live software agent that can autonomously and continuously evolve itself onthefly during runtime when solving realworld software problems more specifically livesweagent starts with the most basic agent scaffold with only access to bash tools eg minisweagent and autonomously evolves its own scaffold implementation while solving realworld software problems our evaluation on the widely studied swebench verified benchmark shows that livesweagent can achieve an impressive solve rate of  without testtime scaling outperforming all existing opensource software agents and approaching the performance of the best proprietary solution moreover livesweagent outperforms stateoftheart manually crafted software agents on the recent swebench pro benchmark achieving the bestknown solve rate of",
    "tokenized_abstract": [
      "large",
      "language",
      "model",
      "llm",
      "reshape",
      "almost",
      "industry",
      "include",
      "software",
      "engineering",
      "recent",
      "year",
      "number",
      "llm",
      "agent",
      "propose",
      "solve",
      "realworld",
      "software",
      "problem",
      "software",
      "agent",
      "typically",
      "equip",
      "suite",
      "coding",
      "tool",
      "autonomously",
      "decide",
      "next",
      "action",
      "form",
      "complete",
      "trajectory",
      "solve",
      "endtoend",
      "software",
      "task",
      "promise",
      "typically",
      "require",
      "dedicated",
      "design",
      "may",
      "still",
      "suboptimal",
      "since",
      "extremely",
      "challenging",
      "costly",
      "exhaust",
      "entire",
      "agent",
      "scaffold",
      "design",
      "space",
      "recognize",
      "software",
      "agent",
      "inherently",
      "software",
      "refinedmodified",
      "researcher",
      "propose",
      "number",
      "selfimproving",
      "software",
      "agent",
      "recently",
      "include",
      "darwingdel",
      "machine",
      "dgm",
      "meanwhile",
      "selfimproving",
      "agent",
      "require",
      "costly",
      "offline",
      "training",
      "specific",
      "benchmark",
      "may",
      "generalize",
      "well",
      "across",
      "different",
      "llm",
      "benchmark",
      "paper",
      "propose",
      "livesweagent",
      "first",
      "live",
      "software",
      "agent",
      "autonomously",
      "continuously",
      "evolve",
      "onthefly",
      "runtime",
      "solve",
      "realworld",
      "software",
      "problem",
      "specifically",
      "livesweagent",
      "start",
      "basic",
      "agent",
      "scaffold",
      "access",
      "bash",
      "tool",
      "eg",
      "minisweagent",
      "autonomously",
      "evolve",
      "scaffold",
      "implementation",
      "solve",
      "realworld",
      "software",
      "problem",
      "evaluation",
      "widely",
      "study",
      "swebench",
      "verify",
      "benchmark",
      "show",
      "livesweagent",
      "achieve",
      "impressive",
      "solve",
      "rate",
      "without",
      "testtime",
      "scale",
      "outperform",
      "exist",
      "opensource",
      "software",
      "agent",
      "approach",
      "performance",
      "best",
      "proprietary",
      "solution",
      "moreover",
      "livesweagent",
      "outperforms",
      "stateoftheart",
      "manually",
      "craft",
      "software",
      "agent",
      "recent",
      "swebench",
      "pro",
      "benchmark",
      "achieve",
      "bestknown",
      "solve",
      "rate"
    ],
    "bm25_score": 1.9101,
    "is_selected": 1
  },
  {
    "id": "2511.13703v1",
    "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
    "original_abstract": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
    "cleaned_abstract": "hospitals and healthcare systems rely on operational decisions that determine patient flow cost and quality of care despite strong performance on medical knowledge and conversational benchmarks foundation models trained on general text may lack the specialized knowledge required for these operational decisions we introduce lang a family of models mb parameters pretrained on a specialized corpus blending b clinical tokens from nyu langone healths ehrs and b tokens from the internet to rigorously evaluate lang in realworld settings we developed the realistic medical evaluation remede a benchmark derived from  ehr notes that evaluates five critical tasks day readmission prediction day mortality prediction length of stay comorbidity coding and predicting insurance claims denial in zeroshot settings both generalpurpose and specialized models underperform on four of five tasks  auroc with mortality prediction being an exception after finetuning langb outperforms finetuned generalist models up to x larger and zeroshot models up to x larger improving auroc by  and  respectively we also observed crosstask scaling with joint finetuning on multiple tasks leading to improvement on other tasks langb effectively transfers to outofdistribution settings including other clinical tasks and an external health system our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning and that this finetuning process is made more efficient by indomain pretraining on ehr our findings support the emerging view that specialized llms can compete with generalist models in specialized tasks and show that effective healthcare systems ai requires the combination of indomain pretraining supervised finetuning and realworld evaluation beyond proxy benchmarks",
    "tokenized_abstract": [
      "hospital",
      "healthcare",
      "system",
      "rely",
      "operational",
      "decision",
      "determine",
      "patient",
      "flow",
      "cost",
      "quality",
      "care",
      "despite",
      "strong",
      "performance",
      "medical",
      "knowledge",
      "conversational",
      "benchmark",
      "foundation",
      "model",
      "train",
      "general",
      "text",
      "may",
      "lack",
      "specialized",
      "knowledge",
      "require",
      "operational",
      "decision",
      "introduce",
      "lang",
      "family",
      "model",
      "mb",
      "parameter",
      "pretrained",
      "specialized",
      "corpus",
      "blending",
      "clinical",
      "token",
      "nyu",
      "langone",
      "healths",
      "ehrs",
      "token",
      "internet",
      "rigorously",
      "evaluate",
      "lang",
      "realworld",
      "setting",
      "develop",
      "realistic",
      "medical",
      "evaluation",
      "remede",
      "benchmark",
      "derive",
      "ehr",
      "note",
      "evaluate",
      "five",
      "critical",
      "task",
      "day",
      "readmission",
      "prediction",
      "day",
      "mortality",
      "prediction",
      "length",
      "stay",
      "comorbidity",
      "coding",
      "predict",
      "insurance",
      "claim",
      "denial",
      "zeroshot",
      "setting",
      "generalpurpose",
      "specialized",
      "model",
      "underperform",
      "four",
      "five",
      "task",
      "auroc",
      "mortality",
      "prediction",
      "exception",
      "finetuning",
      "langb",
      "outperforms",
      "finetuned",
      "generalist",
      "model",
      "large",
      "zeroshot",
      "model",
      "large",
      "improve",
      "auroc",
      "respectively",
      "also",
      "observe",
      "crosstask",
      "scale",
      "joint",
      "finetuning",
      "multiple",
      "task",
      "lead",
      "improvement",
      "task",
      "langb",
      "effectively",
      "transfer",
      "outofdistribution",
      "setting",
      "include",
      "clinical",
      "task",
      "external",
      "health",
      "system",
      "finding",
      "suggest",
      "predictive",
      "capability",
      "hospital",
      "operation",
      "require",
      "explicit",
      "supervise",
      "finetuning",
      "finetuning",
      "process",
      "make",
      "efficient",
      "indomain",
      "pretraining",
      "ehr",
      "finding",
      "support",
      "emerge",
      "view",
      "specialize",
      "llm",
      "compete",
      "generalist",
      "model",
      "specialized",
      "task",
      "show",
      "effective",
      "healthcare",
      "system",
      "ai",
      "require",
      "combination",
      "indomain",
      "pretraining",
      "supervise",
      "finetuning",
      "realworld",
      "evaluation",
      "beyond",
      "proxy",
      "benchmark"
    ],
    "bm25_score": 1.604,
    "is_selected": 1
  },
  {
    "id": "2511.13587v1",
    "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
    "original_abstract": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of 2.8 relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
    "cleaned_abstract": "visual autoregressive ar generation models have demonstrated strong potential for image generation yet their nexttokenprediction paradigm introduces considerable inference latency although speculative decoding sd has been proven effective for accelerating visual ar models its draft one step then verify one step paradigm prevents a direct reduction of the forward passes thus restricting acceleration potential motivated by the visual token interchangeability we for the first time to explore verification skipping in the sd process of visual ar model generation to explicitly cut the number of target model forward passes thereby reducing inference latency based on an analysis of the drafting stages characteristics we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verificationfree steps inspired by these two observations we propose a novel sd framework vvs to accelerate visual ar generation via partial verification skipping which integrates three complementary modules  a verificationfree token selector with dynamical truncation  tokenlevel feature caching and reuse and  finegrained skipped step scheduling consequently vvs reduces the number of target model forward passes by a factor of  relative to vanilla ar decoding while maintaining competitive generation quality offering a superior speedquality tradeoff over conventional sd frameworks and revealing strong potential to reshape the sd paradigm",
    "tokenized_abstract": [
      "visual",
      "autoregressive",
      "ar",
      "generation",
      "model",
      "demonstrate",
      "strong",
      "potential",
      "image",
      "generation",
      "yet",
      "nexttokenprediction",
      "paradigm",
      "introduces",
      "considerable",
      "inference",
      "latency",
      "although",
      "speculative",
      "decode",
      "sd",
      "proven",
      "effective",
      "accelerate",
      "visual",
      "ar",
      "model",
      "draft",
      "one",
      "step",
      "verify",
      "one",
      "step",
      "paradigm",
      "prevent",
      "direct",
      "reduction",
      "forward",
      "pass",
      "thus",
      "restrict",
      "acceleration",
      "potential",
      "motivate",
      "visual",
      "token",
      "interchangeability",
      "first",
      "time",
      "explore",
      "verification",
      "skip",
      "sd",
      "process",
      "visual",
      "ar",
      "model",
      "generation",
      "explicitly",
      "cut",
      "number",
      "target",
      "model",
      "forward",
      "pass",
      "thereby",
      "reduce",
      "inference",
      "latency",
      "base",
      "analysis",
      "drafting",
      "stage",
      "characteristics",
      "observe",
      "verification",
      "redundancy",
      "stale",
      "feature",
      "reusability",
      "key",
      "factor",
      "retain",
      "generation",
      "quality",
      "speedup",
      "verificationfree",
      "step",
      "inspire",
      "two",
      "observation",
      "propose",
      "novel",
      "sd",
      "framework",
      "vvs",
      "accelerate",
      "visual",
      "ar",
      "generation",
      "via",
      "partial",
      "verification",
      "skip",
      "integrate",
      "three",
      "complementary",
      "module",
      "verificationfree",
      "token",
      "selector",
      "dynamical",
      "truncation",
      "tokenlevel",
      "feature",
      "caching",
      "reuse",
      "finegrained",
      "skipped",
      "step",
      "schedule",
      "consequently",
      "vvs",
      "reduce",
      "number",
      "target",
      "model",
      "forward",
      "pass",
      "factor",
      "relative",
      "vanilla",
      "ar",
      "decode",
      "maintain",
      "competitive",
      "generation",
      "quality",
      "offer",
      "superior",
      "speedquality",
      "tradeoff",
      "conventional",
      "sd",
      "framework",
      "reveal",
      "strong",
      "potential",
      "reshape",
      "sd",
      "paradigm"
    ],
    "bm25_score": 1.5245,
    "is_selected": 1
  },
  {
    "id": "2511.13588v1",
    "title": "Data-driven Acceleration of MPC with Guarantees",
    "original_abstract": "Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.",
    "cleaned_abstract": "model predictive control mpc is a powerful framework for optimal control but can be too slow for lowlatency applications we present a datadriven framework to accelerate mpc by replacing online optimization with a nonparametric policy constructed from offline mpc solutions our policy is greedy with respect to a constructed upper bound on the optimal costtogo and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving mpc online our analysis shows that under sufficient coverage condition of the offline data the policy is recursively feasible and admits provable bounded optimality gap these conditions establish an explicit tradeoff between the amount of data collected and the tightness of the bounds our experiments show that this policy is between  and  times faster than standard mpc with only a modest hit to optimality showing potential for realtime control tasks",
    "tokenized_abstract": [
      "model",
      "predictive",
      "control",
      "mpc",
      "powerful",
      "framework",
      "optimal",
      "control",
      "slow",
      "lowlatency",
      "application",
      "present",
      "datadriven",
      "framework",
      "accelerate",
      "mpc",
      "replace",
      "online",
      "optimization",
      "nonparametric",
      "policy",
      "construct",
      "offline",
      "mpc",
      "solution",
      "policy",
      "greedy",
      "respect",
      "construct",
      "upper",
      "bound",
      "optimal",
      "costtogo",
      "implement",
      "nonparametric",
      "lookup",
      "rule",
      "order",
      "magnitude",
      "faster",
      "solve",
      "mpc",
      "online",
      "analysis",
      "show",
      "sufficient",
      "coverage",
      "condition",
      "offline",
      "data",
      "policy",
      "recursively",
      "feasible",
      "admit",
      "provable",
      "bound",
      "optimality",
      "gap",
      "condition",
      "establish",
      "explicit",
      "tradeoff",
      "amount",
      "data",
      "collect",
      "tightness",
      "bound",
      "experiment",
      "show",
      "policy",
      "time",
      "faster",
      "standard",
      "mpc",
      "modest",
      "hit",
      "optimality",
      "show",
      "potential",
      "realtime",
      "control",
      "task"
    ],
    "bm25_score": 1.5124,
    "is_selected": 1
  },
  {
    "id": "2511.13685v1",
    "title": "Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers",
    "original_abstract": "In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.",
    "cleaned_abstract": "in this study we tackle the challenging task of predicting secondary structures from protein primary sequences a pivotal initial stride towards predicting tertiary structures while yielding crucial insights into protein activity relationships and functions existing methods often utilize extensive sets of unlabeled amino acid sequences however these approaches neither explicitly capture nor harness the accessible protein d structural data which is recognized as a decisive factor in dictating protein functions to address this we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information we adeptly combine graph neural networks gnns and language models lms specifically utilizing a pretrained transformerbased protein language model to encode amino acid sequences and employing messagepassing mechanisms like gcn and rgcn to capture geometric characteristics of protein structures employing convolution within a specific nodes nearby region including relations we stack multiple convolutional layers to efficiently learn combined insights from the proteins spatial graph revealing intricate interconnections and dependencies in its structural arrangement to assess our models performance we employed the training dataset provided by netsurfp which outlines secondary structure in and states extensive experiments show that our proposed model ssrgnet surpasses the baseline on fscores",
    "tokenized_abstract": [
      "study",
      "tackle",
      "challenge",
      "task",
      "predict",
      "secondary",
      "structure",
      "protein",
      "primary",
      "sequence",
      "pivotal",
      "initial",
      "stride",
      "towards",
      "predict",
      "tertiary",
      "structure",
      "yield",
      "crucial",
      "insight",
      "protein",
      "activity",
      "relationship",
      "function",
      "exist",
      "method",
      "often",
      "utilize",
      "extensive",
      "set",
      "unlabeled",
      "amino",
      "acid",
      "sequence",
      "however",
      "approach",
      "neither",
      "explicitly",
      "capture",
      "harness",
      "accessible",
      "protein",
      "structural",
      "data",
      "recognize",
      "decisive",
      "factor",
      "dictate",
      "protein",
      "function",
      "address",
      "utilize",
      "protein",
      "residue",
      "graph",
      "introduce",
      "various",
      "form",
      "sequential",
      "structural",
      "connection",
      "capture",
      "enhanced",
      "spatial",
      "information",
      "adeptly",
      "combine",
      "graph",
      "neural",
      "network",
      "gnns",
      "language",
      "model",
      "lms",
      "specifically",
      "utilize",
      "pretrained",
      "transformerbased",
      "protein",
      "language",
      "model",
      "encode",
      "amino",
      "acid",
      "sequence",
      "employ",
      "messagepassing",
      "mechanism",
      "like",
      "gcn",
      "rgcn",
      "capture",
      "geometric",
      "characteristic",
      "protein",
      "structure",
      "employ",
      "convolution",
      "within",
      "specific",
      "node",
      "nearby",
      "region",
      "include",
      "relation",
      "stack",
      "multiple",
      "convolutional",
      "layer",
      "efficiently",
      "learn",
      "combined",
      "insight",
      "protein",
      "spatial",
      "graph",
      "reveal",
      "intricate",
      "interconnection",
      "dependency",
      "structural",
      "arrangement",
      "assess",
      "model",
      "performance",
      "employ",
      "training",
      "dataset",
      "provide",
      "netsurfp",
      "outline",
      "secondary",
      "structure",
      "state",
      "extensive",
      "experiment",
      "show",
      "propose",
      "model",
      "ssrgnet",
      "surpass",
      "baseline",
      "fscores"
    ],
    "bm25_score": 1.4244,
    "is_selected": 1
  },
  {
    "id": "2511.13545v1",
    "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks",
    "original_abstract": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation \"oracle\" is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.",
    "cleaned_abstract": "the advent of multimodal deep learning models such as clip has unlocked new frontiers in a wide range of applications from imagetext understanding to classification tasks however these models are not safe for adversarial attacks particularly backdoor attacks which can subtly manipulate model behavior moreover existing defense methods typically involve training from scratch or finetuning using a large dataset without pinpointing the specific labels that are affected in this study we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks in particular given a poisoned clip model our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner to that end an image segmentation oracle is introduced as the supervisor for the output of the poisoned clip we develop two algorithms to rectify the poisoned model  differentiating between clip and oracles knowledge to identify potential triggers  pinpointing affected labels and victim samples and curating a compact finetuning dataset with this knowledge we are allowed to rectify the poisoned clip model to negate backdoor effects extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in clipbased backdoor defense",
    "tokenized_abstract": [
      "advent",
      "multimodal",
      "deep",
      "learning",
      "model",
      "clip",
      "unlock",
      "new",
      "frontier",
      "wide",
      "range",
      "application",
      "imagetext",
      "understanding",
      "classification",
      "task",
      "however",
      "model",
      "safe",
      "adversarial",
      "attack",
      "particularly",
      "backdoor",
      "attack",
      "subtly",
      "manipulate",
      "model",
      "behavior",
      "moreover",
      "exist",
      "defense",
      "method",
      "typically",
      "involve",
      "train",
      "scratch",
      "finetuning",
      "use",
      "large",
      "dataset",
      "without",
      "pinpoint",
      "specific",
      "label",
      "affect",
      "study",
      "introduce",
      "innovative",
      "strategy",
      "enhance",
      "robustness",
      "multimodal",
      "contrastive",
      "learning",
      "model",
      "attack",
      "particular",
      "give",
      "poisoned",
      "clip",
      "model",
      "approach",
      "identify",
      "backdoor",
      "trigger",
      "pinpoint",
      "victim",
      "sample",
      "label",
      "efficient",
      "manner",
      "end",
      "image",
      "segmentation",
      "oracle",
      "introduce",
      "supervisor",
      "output",
      "poisoned",
      "clip",
      "develop",
      "two",
      "algorithm",
      "rectify",
      "poisoned",
      "model",
      "differentiate",
      "clip",
      "oracle",
      "knowledge",
      "identify",
      "potential",
      "trigger",
      "pinpoint",
      "affected",
      "label",
      "victim",
      "sample",
      "curating",
      "compact",
      "finetuning",
      "dataset",
      "knowledge",
      "allow",
      "rectify",
      "poisoned",
      "clip",
      "model",
      "negate",
      "backdoor",
      "effect",
      "extensive",
      "experiment",
      "visual",
      "recognition",
      "benchmark",
      "demonstrate",
      "strategy",
      "effective",
      "clipbased",
      "backdoor",
      "defense"
    ],
    "bm25_score": 1.3926,
    "is_selected": 1
  },
  {
    "id": "2511.13653v1",
    "title": "Weight-sparse transformers have interpretable circuits",
    "original_abstract": "Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.",
    "cleaned_abstract": "finding humanunderstandable circuits in language models is a central goal of the field of mechanistic interpretability we train models to have more understandable circuits by constraining most of their weights to be zeros so that each neuron only has a few connections to recover finegrained circuits underlying each of several handcrafted tasks we prune the models to isolate the part responsible for the task these circuits often contain neurons and residual channels that correspond to natural concepts with a small number of straightforwardly interpretable connections between them we study how these models scale and find that making weights sparser trades off capability for interpretability and scaling model size improves the capabilityinterpretability frontier however scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge in addition to training weightsparse models de novo we show preliminary results suggesting our method can also be adapted to explain existing dense models our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor",
    "tokenized_abstract": [
      "find",
      "humanunderstandable",
      "circuit",
      "language",
      "model",
      "central",
      "goal",
      "field",
      "mechanistic",
      "interpretability",
      "train",
      "model",
      "understandable",
      "circuit",
      "constrain",
      "weight",
      "zero",
      "neuron",
      "connection",
      "recover",
      "finegrained",
      "circuit",
      "underlie",
      "several",
      "handcrafted",
      "task",
      "prune",
      "model",
      "isolate",
      "part",
      "responsible",
      "task",
      "circuit",
      "often",
      "contain",
      "neuron",
      "residual",
      "channel",
      "correspond",
      "natural",
      "concept",
      "small",
      "number",
      "straightforwardly",
      "interpretable",
      "connection",
      "study",
      "model",
      "scale",
      "find",
      "make",
      "weight",
      "sparser",
      "trade",
      "capability",
      "interpretability",
      "scale",
      "model",
      "size",
      "improve",
      "capabilityinterpretability",
      "frontier",
      "however",
      "scale",
      "sparse",
      "model",
      "beyond",
      "ten",
      "million",
      "nonzero",
      "parameter",
      "preserve",
      "interpretability",
      "remain",
      "challenge",
      "addition",
      "train",
      "weightsparse",
      "model",
      "de",
      "novo",
      "show",
      "preliminary",
      "result",
      "suggest",
      "method",
      "also",
      "adapt",
      "explain",
      "exist",
      "dense",
      "model",
      "work",
      "produce",
      "circuit",
      "achieve",
      "unprecedented",
      "level",
      "human",
      "understandability",
      "validate",
      "considerable",
      "rigor"
    ],
    "bm25_score": 1.2719,
    "is_selected": 1
  },
  {
    "id": "2511.13598v1",
    "title": "Robust Client-Server Watermarking for Split Federated Learning",
    "original_abstract": "Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over 95% watermark detection rate (p-value < 0.03) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.",
    "cleaned_abstract": "split federated learning sfl is renowned for its privacypreserving nature and low computational overhead among decentralized machine learning paradigms in this framework clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation however sfl is a doubleedged sword while it enables edge computing and enhances privacy it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training existing watermarking techniques fail to protect both sides since no single participant possesses the complete model to address this we propose rise a robust model intellectual property protection scheme using clientserver watermark embedding for sfl specifically rise adopts an asymmetric clientserver watermarking design the server embeds featurebased watermarks through a loss regularization term while clients embed backdoorbased watermarks by injecting predefined trigger samples into private datasets this coembedding strategy enables both clients and the server to verify model ownership experimental results on standard datasets and multiple network architectures show that rise achieves over  watermark detection rate pvalue   across most settings it exhibits no mutual interference between client and serverside watermarks and remains robust against common removal attacks",
    "tokenized_abstract": [
      "split",
      "federate",
      "learning",
      "sfl",
      "renowned",
      "privacypreserving",
      "nature",
      "low",
      "computational",
      "overhead",
      "among",
      "decentralized",
      "machine",
      "learn",
      "paradigm",
      "framework",
      "client",
      "employ",
      "lightweight",
      "model",
      "process",
      "private",
      "data",
      "locally",
      "transmit",
      "intermediate",
      "output",
      "powerful",
      "server",
      "computation",
      "however",
      "sfl",
      "doubleedged",
      "sword",
      "enable",
      "edge",
      "compute",
      "enhances",
      "privacy",
      "also",
      "introduce",
      "intellectual",
      "property",
      "ambiguity",
      "client",
      "server",
      "jointly",
      "contribute",
      "train",
      "exist",
      "watermarking",
      "technique",
      "fail",
      "protect",
      "side",
      "since",
      "single",
      "participant",
      "possess",
      "complete",
      "model",
      "address",
      "propose",
      "rise",
      "robust",
      "model",
      "intellectual",
      "property",
      "protection",
      "scheme",
      "use",
      "clientserver",
      "watermark",
      "embed",
      "sfl",
      "specifically",
      "rise",
      "adopts",
      "asymmetric",
      "clientserver",
      "watermarking",
      "design",
      "server",
      "embeds",
      "featurebased",
      "watermark",
      "loss",
      "regularization",
      "term",
      "client",
      "embed",
      "backdoorbased",
      "watermark",
      "inject",
      "predefined",
      "trigger",
      "sample",
      "private",
      "datasets",
      "coembedding",
      "strategy",
      "enable",
      "client",
      "server",
      "verify",
      "model",
      "ownership",
      "experimental",
      "result",
      "standard",
      "datasets",
      "multiple",
      "network",
      "architecture",
      "show",
      "rise",
      "achieve",
      "watermark",
      "detection",
      "rate",
      "pvalue",
      "across",
      "setting",
      "exhibit",
      "mutual",
      "interference",
      "client",
      "serverside",
      "watermark",
      "remain",
      "robust",
      "common",
      "removal",
      "attack"
    ],
    "bm25_score": 1.2542,
    "is_selected": 1
  },
  {
    "id": "2511.13702v1",
    "title": "ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification",
    "original_abstract": "Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.",
    "cleaned_abstract": "travel mode identification tmi from gps trajectories is critical for urban intelligence but is hampered by the high cost of annotation leading to severe label scarcity prevailing semisupervised learning ssl methods are illsuited for this task as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold we propose stproc a novel graphprototypical multiobjective ssl framework to address these limitations our framework synergizes a graphprototypical core with foundational ssl support the core exploits the data manifold via graph regularization prototypical anchoring and a novel marginaware pseudolabeling strategy to actively reject noise this core is supported and stabilized by foundational contrastive and teacherstudent consistency losses ensuring highquality representations and robust optimization stproc outperforms all baselines by a significant margin demonstrating its efficacy in realworld sparselabel settings with a performance boost of  over stateoftheart methods like fixmatch",
    "tokenized_abstract": [
      "travel",
      "mode",
      "identification",
      "tmi",
      "gps",
      "trajectory",
      "critical",
      "urban",
      "intelligence",
      "hamper",
      "high",
      "cost",
      "annotation",
      "lead",
      "severe",
      "label",
      "scarcity",
      "prevail",
      "semisupervised",
      "learn",
      "ssl",
      "method",
      "illsuited",
      "task",
      "suffer",
      "catastrophic",
      "confirmation",
      "bias",
      "ignore",
      "intrinsic",
      "data",
      "manifold",
      "propose",
      "stproc",
      "novel",
      "graphprototypical",
      "multiobjective",
      "ssl",
      "framework",
      "address",
      "limitation",
      "framework",
      "synergizes",
      "graphprototypical",
      "core",
      "foundational",
      "ssl",
      "support",
      "core",
      "exploit",
      "data",
      "manifold",
      "via",
      "graph",
      "regularization",
      "prototypical",
      "anchoring",
      "novel",
      "marginaware",
      "pseudolabeling",
      "strategy",
      "actively",
      "reject",
      "noise",
      "core",
      "support",
      "stabilize",
      "foundational",
      "contrastive",
      "teacherstudent",
      "consistency",
      "loss",
      "ensure",
      "highquality",
      "representation",
      "robust",
      "optimization",
      "stproc",
      "outperform",
      "baseline",
      "significant",
      "margin",
      "demonstrate",
      "efficacy",
      "realworld",
      "sparselabel",
      "setting",
      "performance",
      "boost",
      "stateoftheart",
      "method",
      "like",
      "fixmatch"
    ],
    "bm25_score": 1.2385,
    "is_selected": 1
  },
  {
    "id": "2511.13719v1",
    "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
    "original_abstract": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
    "cleaned_abstract": "despite remarkable progress multimodal foundation models still exhibit surprising deficiencies in spatial intelligence in this work we explore scaling up multimodal foundation models to cultivate spatial intelligence within the sensenovasi family built upon established multimodal foundations including visual understanding models ie qwenvl and internvl and unified understanding and generation models ie bagel we take a principled approach to constructing highperforming and robust spatial intelligence by systematically curating sensenovasim eight million diverse data samples under a rigorous taxonomy of spatial capabilities sensenovasi demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks  on vsibench  on mmsi  on mindcube  on viewspatial and  on site while maintaining strong general multimodal understanding eg  on mmbenchen more importantly we analyze the impact of data scaling discuss early signs of emergent generalization capabilities enabled by diverse data training analyze the risk of overfitting and language shortcuts present a preliminary study on spatial chainofthought reasoning and validate the potential downstream application sensenovasi is an ongoing project and this report will be updated continuously all newly trained multimodal foundation models are publicly released to facilitate further research in this direction",
    "tokenized_abstract": [
      "despite",
      "remarkable",
      "progress",
      "multimodal",
      "foundation",
      "model",
      "still",
      "exhibit",
      "surprising",
      "deficiency",
      "spatial",
      "intelligence",
      "work",
      "explore",
      "scale",
      "multimodal",
      "foundation",
      "model",
      "cultivate",
      "spatial",
      "intelligence",
      "within",
      "sensenovasi",
      "family",
      "build",
      "upon",
      "establish",
      "multimodal",
      "foundation",
      "include",
      "visual",
      "understanding",
      "model",
      "ie",
      "qwenvl",
      "internvl",
      "unified",
      "understanding",
      "generation",
      "model",
      "ie",
      "bagel",
      "take",
      "principled",
      "approach",
      "construct",
      "highperforming",
      "robust",
      "spatial",
      "intelligence",
      "systematically",
      "curating",
      "sensenovasim",
      "eight",
      "million",
      "diverse",
      "data",
      "sample",
      "rigorous",
      "taxonomy",
      "spatial",
      "capability",
      "sensenovasi",
      "demonstrates",
      "unprecedented",
      "performance",
      "across",
      "broad",
      "range",
      "spatial",
      "intelligence",
      "benchmark",
      "vsibench",
      "mmsi",
      "mindcube",
      "viewspatial",
      "site",
      "maintain",
      "strong",
      "general",
      "multimodal",
      "understand",
      "eg",
      "mmbenchen",
      "importantly",
      "analyze",
      "impact",
      "data",
      "scale",
      "discuss",
      "early",
      "sign",
      "emergent",
      "generalization",
      "capability",
      "enable",
      "diverse",
      "data",
      "training",
      "analyze",
      "risk",
      "overfitting",
      "language",
      "shortcut",
      "present",
      "preliminary",
      "study",
      "spatial",
      "chainofthought",
      "reasoning",
      "validate",
      "potential",
      "downstream",
      "application",
      "sensenovasi",
      "ongoing",
      "project",
      "report",
      "update",
      "continuously",
      "newly",
      "train",
      "multimodal",
      "foundation",
      "model",
      "publicly",
      "release",
      "facilitate",
      "research",
      "direction"
    ],
    "bm25_score": 1.1999,
    "is_selected": 1
  },
  {
    "id": "2511.13595v1",
    "title": "Physics-Informed Neural Networks for Nonlinear Output Regulation",
    "original_abstract": "This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold (w) and a feedforward input c(w) that render such manifold invariant. The pair ((w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates (w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.",
    "cleaned_abstract": "this work addresses the fullinformation output regulation problem for nonlinear systems assuming the states of both the plant and the exosystem are known in this setting perfect tracking or rejection is achieved by constructing a zeroregulationerror manifold w and a feedforward input cw that render such manifold invariant the pair w cw is characterized by the regulator equations ie a system of pdes with an algebraic constraint we focus on accurately solving the regulator equations introducing a physicsinformed neural network pinn approach that directly approximates w and cw by minimizing the residuals under boundary and feasibility conditions without requiring precomputed trajectories or labeled data the learned operator maps exosystem states to steady state plant states and inputs enables realtime inference and critically generalizes across families of the exosystem with varying initial conditions and parameters the framework is validated on a regulation task that synchronizes a helicopters vertical dynamics with a harmonically oscillating platform the resulting pinnbased solver reconstructs the zeroerror manifold with high fidelity and sustains regulation performance under exosystem variations highlighting the potential of learningenabled solvers for nonlinear output regulation the proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem",
    "tokenized_abstract": [
      "work",
      "address",
      "fullinformation",
      "output",
      "regulation",
      "problem",
      "nonlinear",
      "system",
      "assume",
      "state",
      "plant",
      "exosystem",
      "know",
      "setting",
      "perfect",
      "tracking",
      "rejection",
      "achieve",
      "construct",
      "zeroregulationerror",
      "manifold",
      "w",
      "feedforward",
      "input",
      "cw",
      "render",
      "manifold",
      "invariant",
      "pair",
      "w",
      "cw",
      "characterize",
      "regulator",
      "equation",
      "ie",
      "system",
      "pdes",
      "algebraic",
      "constraint",
      "focus",
      "accurately",
      "solve",
      "regulator",
      "equation",
      "introduce",
      "physicsinformed",
      "neural",
      "network",
      "pinn",
      "approach",
      "directly",
      "approximate",
      "w",
      "cw",
      "minimize",
      "residual",
      "boundary",
      "feasibility",
      "condition",
      "without",
      "require",
      "precomputed",
      "trajectory",
      "label",
      "data",
      "learn",
      "operator",
      "map",
      "exosystem",
      "state",
      "steady",
      "state",
      "plant",
      "state",
      "input",
      "enables",
      "realtime",
      "inference",
      "critically",
      "generalizes",
      "across",
      "family",
      "exosystem",
      "vary",
      "initial",
      "condition",
      "parameter",
      "framework",
      "validate",
      "regulation",
      "task",
      "synchronize",
      "helicopter",
      "vertical",
      "dynamic",
      "harmonically",
      "oscillate",
      "platform",
      "result",
      "pinnbased",
      "solver",
      "reconstruct",
      "zeroerror",
      "manifold",
      "high",
      "fidelity",
      "sustain",
      "regulation",
      "performance",
      "exosystem",
      "variation",
      "highlight",
      "potential",
      "learningenabled",
      "solver",
      "nonlinear",
      "output",
      "regulation",
      "propose",
      "approach",
      "broadly",
      "applicable",
      "nonlinear",
      "system",
      "admit",
      "solution",
      "output",
      "regulation",
      "problem"
    ],
    "bm25_score": 0.9002,
    "is_selected": 1
  },
  {
    "id": "2511.13714v1",
    "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
    "original_abstract": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only 6K unlabeled images and 0.02% additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over 11 benchmarks, UnSAMv2 improves NoC (5.69  4.75), 1-IoU (58.0  73.1), and AR (49.6  68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.",
    "cleaned_abstract": "the segment anything model sam family has become a widely adopted vision foundation model but its ability to control segmentation granularity remains limited users often need to refine results manually  by adding more prompts or selecting from pregenerated masks  to achieve the desired level of detail this process can be ambiguous as the same prompt may correspond to several plausible masks and collecting dense annotations across all granularities is prohibitively expensive making supervised solutions infeasible to address this limitation we introduce unsamv which enables segment anything at any granularity without human annotations unsamv extends the divideandconquer strategy of unsam by discovering abundant maskgranularity pairs and introducing a novel granularity control embedding that enables precise continuous control over segmentation scale remarkably with only k unlabeled images and  additional parameters unsamv substantially enhances sam achieving segment anything at any granularity across interactive wholeimage and video segmentation tasks evaluated on over  benchmarks unsamv improves noc    iou    and ar    showing that small amounts of unlabeled data with a granularityaware selfsupervised learning method can unlock the potential of vision foundation models",
    "tokenized_abstract": [
      "segment",
      "anything",
      "model",
      "sam",
      "family",
      "become",
      "widely",
      "adopt",
      "vision",
      "foundation",
      "model",
      "ability",
      "control",
      "segmentation",
      "granularity",
      "remain",
      "limited",
      "user",
      "often",
      "need",
      "refine",
      "result",
      "manually",
      "add",
      "prompt",
      "select",
      "pregenerated",
      "mask",
      "achieve",
      "desired",
      "level",
      "detail",
      "process",
      "ambiguous",
      "prompt",
      "may",
      "correspond",
      "several",
      "plausible",
      "mask",
      "collect",
      "dense",
      "annotation",
      "across",
      "granularity",
      "prohibitively",
      "expensive",
      "making",
      "supervised",
      "solution",
      "infeasible",
      "address",
      "limitation",
      "introduce",
      "unsamv",
      "enable",
      "segment",
      "anything",
      "granularity",
      "without",
      "human",
      "annotation",
      "unsamv",
      "extend",
      "divideandconquer",
      "strategy",
      "unsam",
      "discover",
      "abundant",
      "maskgranularity",
      "pair",
      "introduce",
      "novel",
      "granularity",
      "control",
      "embed",
      "enable",
      "precise",
      "continuous",
      "control",
      "segmentation",
      "scale",
      "remarkably",
      "unlabeled",
      "image",
      "additional",
      "parameter",
      "unsamv",
      "substantially",
      "enhance",
      "sam",
      "achieve",
      "segment",
      "anything",
      "granularity",
      "across",
      "interactive",
      "wholeimage",
      "video",
      "segmentation",
      "task",
      "evaluate",
      "benchmark",
      "unsamv",
      "improves",
      "noc",
      "iou",
      "ar",
      "showing",
      "small",
      "amount",
      "unlabeled",
      "data",
      "granularityaware",
      "selfsupervised",
      "learn",
      "method",
      "unlock",
      "potential",
      "vision",
      "foundation",
      "model"
    ],
    "bm25_score": 0.3447,
    "is_selected": 1
  },
  {
    "id": "2511.13625v1",
    "title": "Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization",
    "original_abstract": "Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.",
    "cleaned_abstract": "bayesian optimization bo efficiently finds highperforming parameters by maximizing an acquisition function which models the promise of parameters a major computational bottleneck arises in acquisition function optimization where multistart optimization mso with quasinewton qn methods is required due to the nonconvexity of the acquisition function botorch a widely used bo library currently optimizes the summed acquisition function over multiple points leading to the speedup of mso owing to pytorch batching nevertheless this paper empirically demonstrates the suboptimality of this approach in terms of offdiagonal approximation errors in the inverse hessian of a qn method slowing down its convergence to address this problem we propose to decouple qn updates using a coroutine while batching the acquisition function calls our approach not only yields the theoretically identical convergence to the sequential mso but also drastically reduces the wallclock time compared to the previous approaches",
    "tokenized_abstract": [
      "bayesian",
      "optimization",
      "bo",
      "efficiently",
      "find",
      "highperforming",
      "parameter",
      "maximize",
      "acquisition",
      "function",
      "model",
      "promise",
      "parameter",
      "major",
      "computational",
      "bottleneck",
      "arises",
      "acquisition",
      "function",
      "optimization",
      "multistart",
      "optimization",
      "mso",
      "quasinewton",
      "qn",
      "method",
      "require",
      "due",
      "nonconvexity",
      "acquisition",
      "function",
      "botorch",
      "widely",
      "use",
      "bo",
      "library",
      "currently",
      "optimize",
      "summed",
      "acquisition",
      "function",
      "multiple",
      "point",
      "lead",
      "speedup",
      "mso",
      "owe",
      "pytorch",
      "batch",
      "nevertheless",
      "paper",
      "empirically",
      "demonstrate",
      "suboptimality",
      "approach",
      "term",
      "offdiagonal",
      "approximation",
      "error",
      "inverse",
      "hessian",
      "qn",
      "method",
      "slow",
      "convergence",
      "address",
      "problem",
      "propose",
      "decouple",
      "qn",
      "update",
      "use",
      "coroutine",
      "batch",
      "acquisition",
      "function",
      "call",
      "approach",
      "yield",
      "theoretically",
      "identical",
      "convergence",
      "sequential",
      "mso",
      "also",
      "drastically",
      "reduce",
      "wallclock",
      "time",
      "compare",
      "previous",
      "approach"
    ],
    "bm25_score": 0.2507,
    "is_selected": 0
  },
  {
    "id": "2511.13621v1",
    "title": "Alpha Divergence Losses for Biometric Verification",
    "original_abstract": "Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced -divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when >1). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based -divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.",
    "cleaned_abstract": "performance in face and speaker verification is largely driven by margin based softmax losses like cosface and arcface recently introduced divergence loss functions offer a compelling alternative particularly for their ability to induce sparse solutions when  however integrating an angular margincrucial for verification tasksis not straightforward we find this integration can be achieved in at least two distinct ways via the reference measure prior probabilities or via the logits unnormalized loglikelihoods in this paper we explore both pathways deriving two novel marginbased divergence losses qmargin margin in the reference measure and am margin in the logits we identify and address a critical training instability in amcaused by the interplay of penalized logits and sparsitywith a simple yet effective prototype reinitialization strategy our methods achieve significant performance gains on the challenging ijbb and ijbc face verification benchmarks we demonstrate similarly strong performance in speaker verification on voxceleb crucially our models significantly outperform strong baselines at low false acceptance rates far this capability is crucial for practical highsecurity applications such as banking authentication when minimizing false authentications is paramount",
    "tokenized_abstract": [
      "performance",
      "face",
      "speaker",
      "verification",
      "largely",
      "drive",
      "margin",
      "base",
      "softmax",
      "loss",
      "like",
      "cosface",
      "arcface",
      "recently",
      "introduce",
      "divergence",
      "loss",
      "function",
      "offer",
      "compelling",
      "alternative",
      "particularly",
      "ability",
      "induce",
      "sparse",
      "solution",
      "however",
      "integrate",
      "angular",
      "margincrucial",
      "verification",
      "tasksis",
      "straightforward",
      "find",
      "integration",
      "achieve",
      "least",
      "two",
      "distinct",
      "way",
      "via",
      "reference",
      "measure",
      "prior",
      "probability",
      "via",
      "logits",
      "unnormalized",
      "loglikelihoods",
      "paper",
      "explore",
      "pathway",
      "derive",
      "two",
      "novel",
      "marginbased",
      "divergence",
      "loss",
      "qmargin",
      "margin",
      "reference",
      "measure",
      "margin",
      "logits",
      "identify",
      "address",
      "critical",
      "training",
      "instability",
      "amcaused",
      "interplay",
      "penalized",
      "logits",
      "sparsitywith",
      "simple",
      "yet",
      "effective",
      "prototype",
      "reinitialization",
      "strategy",
      "method",
      "achieve",
      "significant",
      "performance",
      "gain",
      "challenging",
      "ijbb",
      "ijbc",
      "face",
      "verification",
      "benchmark",
      "demonstrate",
      "similarly",
      "strong",
      "performance",
      "speaker",
      "verification",
      "voxceleb",
      "crucially",
      "model",
      "significantly",
      "outperform",
      "strong",
      "baseline",
      "low",
      "false",
      "acceptance",
      "rate",
      "far",
      "capability",
      "crucial",
      "practical",
      "highsecurity",
      "application",
      "banking",
      "authentication",
      "minimize",
      "false",
      "authentication",
      "paramount"
    ],
    "bm25_score": 0.2373,
    "is_selected": 0
  },
  {
    "id": "2511.13530v1",
    "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety",
    "original_abstract": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.",
    "cleaned_abstract": "social anxiety is a prevalent condition that affects interpersonal interactions and social functioning recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the humanrobot interaction context accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets where each signal modality provides complementary insights into its manifestations however such datasets remain scarce limiting progress in both research and applications to address this this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a humanrobot interaction context the dataset will consist of synchronised audio video and physiological recordings acquired from at least  participants grouped according to their level of social anxiety as they engage in approximately minute interactive wizardofoz roleplay scenarios with the furhat social robot under controlled experimental conditions in addition to multimodal data the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses this work can contribute to research on affectadaptive humanrobot interaction by providing support for robust multimodal detection of social anxiety",
    "tokenized_abstract": [
      "social",
      "anxiety",
      "prevalent",
      "condition",
      "affect",
      "interpersonal",
      "interaction",
      "social",
      "functioning",
      "recent",
      "advance",
      "artificial",
      "intelligence",
      "social",
      "robotics",
      "offer",
      "new",
      "opportunity",
      "examine",
      "social",
      "anxiety",
      "humanrobot",
      "interaction",
      "context",
      "accurate",
      "detection",
      "affective",
      "state",
      "behaviour",
      "associate",
      "social",
      "anxiety",
      "require",
      "multimodal",
      "datasets",
      "signal",
      "modality",
      "provide",
      "complementary",
      "insight",
      "manifestation",
      "however",
      "datasets",
      "remain",
      "scarce",
      "limit",
      "progress",
      "research",
      "application",
      "address",
      "paper",
      "present",
      "protocol",
      "multimodal",
      "dataset",
      "collection",
      "design",
      "reflect",
      "social",
      "anxiety",
      "humanrobot",
      "interaction",
      "context",
      "dataset",
      "consist",
      "synchronised",
      "audio",
      "video",
      "physiological",
      "recording",
      "acquire",
      "least",
      "participant",
      "group",
      "accord",
      "level",
      "social",
      "anxiety",
      "engage",
      "approximately",
      "minute",
      "interactive",
      "wizardofoz",
      "roleplay",
      "scenario",
      "furhat",
      "social",
      "robot",
      "control",
      "experimental",
      "condition",
      "addition",
      "multimodal",
      "data",
      "dataset",
      "enrich",
      "contextual",
      "data",
      "provide",
      "deep",
      "insight",
      "individual",
      "variability",
      "social",
      "anxiety",
      "response",
      "work",
      "contribute",
      "research",
      "affectadaptive",
      "humanrobot",
      "interaction",
      "provide",
      "support",
      "robust",
      "multimodal",
      "detection",
      "social",
      "anxiety"
    ],
    "bm25_score": 0.0,
    "is_selected": 0
  },
  {
    "id": "2511.13525v1",
    "title": "AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions",
    "original_abstract": "Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.",
    "cleaned_abstract": "fairness in artificial intelligence ai has become a growing concern due to discriminatory outcomes in aibased decisionmaking systems while various methods have been proposed to mitigate bias most rely on complete demographic information an assumption often impractical due to legal constraints and the risk of reinforcing discrimination this survey examines fairness in ai when demographics are incomplete addressing the gap between traditional approaches and realworld challenges we introduce a novel taxonomy of fairness notions in this setting clarifying their relationships and distinctions additionally we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field",
    "tokenized_abstract": [
      "fairness",
      "artificial",
      "intelligence",
      "ai",
      "become",
      "grow",
      "concern",
      "due",
      "discriminatory",
      "outcome",
      "aibased",
      "decisionmaking",
      "system",
      "various",
      "method",
      "propose",
      "mitigate",
      "bias",
      "rely",
      "complete",
      "demographic",
      "information",
      "assumption",
      "often",
      "impractical",
      "due",
      "legal",
      "constraint",
      "risk",
      "reinforce",
      "discrimination",
      "survey",
      "examine",
      "fairness",
      "ai",
      "demographic",
      "incomplete",
      "address",
      "gap",
      "traditional",
      "approach",
      "realworld",
      "challenge",
      "introduce",
      "novel",
      "taxonomy",
      "fairness",
      "notion",
      "set",
      "clarify",
      "relationship",
      "distinction",
      "additionally",
      "summarize",
      "exist",
      "technique",
      "promote",
      "fairness",
      "beyond",
      "complete",
      "demographic",
      "highlight",
      "open",
      "research",
      "question",
      "encourage",
      "progress",
      "field"
    ],
    "bm25_score": 0.0,
    "is_selected": 0
  }
]