2511.14136
Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems
针对现有 AI 智能体基准仅关注准确性而忽视企业需求（成本、可靠性等）的问题，提出了 CLEAR 评估框架（涵盖成本、延迟、功效、保障、可靠性）。通过对 6 个主流智能体在 300 项任务上的评估发现，仅优化准确性会导致成本高出 4.4-10.8 倍。CLEAR 框架在预测生产成功率方面与专家评估的相关性（0.83）远高于仅基于准确性的评估（0.41），为企业级 AI 部署提供了更全面的指导。
2511.14017
From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs
研究发现针对特定领域（如网络安全、安全概念）的狭窄拒绝遗忘（Refusal Unlearning）会导致涌现性错位（Emergent Misalignment
2511.14013
Developing a Grounded View of AI
本文从工程角度审视 AI 行为，探讨其与基于规则软件的根本区别。文章主张保护人类追求规则理性的价值，并指出 AI 行为规则虽隐蔽，但可通过所提出的方法论对三种决策类型的 AI 行为进行辨析。这是界定人类责任、确保 AI 系统健全性及造福人类社会的前提。
2511.14565
Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language
针对从演示中学习奖励函数易过拟合无关状态细节的问题，提出了 Masked IRL 框架。该方法利用大语言模型（LLM）结合演示（展示如何行动）与自然语言指令（指定重点）的互补信息。Masked IRL 通过 LLM 从指令推断状态相关性掩码，强制模型对无关状态保持不变性，并利用 LLM 推理在演示上下文中消除指令歧义。仿真和真机实验表明，该方法比现有语言条件 IRL 方法性能提升 15%，且数据需求量减少 4.7 倍，显著提高了样本效率和对歧义语言的鲁棒性。
2511.14591
Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate Reliance and Interacts with Human Base Rate Neglect
本文探讨了 AI 偏差（类别不平衡）与人类偏差（基率忽略）在决策中的复杂交互。通过一项 N=46 的疾病分类实验，参与者使用在平衡或不平衡数据集上训练的 AI 辅助决策。结果发现，类别不平衡破坏了参与者对 AI 依赖的校准能力。更重要的是，观察到类别不平衡与基率忽略之间存在相互增强的效应，证实了复合人机偏差的存在。这表明需要从交互视角研究偏差，关注人机协作中偏差的相互强化作用。